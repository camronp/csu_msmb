[{"authors":null,"categories":null,"content":"Bailey Fosdick is an assistant professor of statistics at Colorado State University. Her primary research interests lie in the development of statistical methods for analyzing network data, with particular attention to applications in ecology and the social sciences. She also studies covariance models for multiway data, Bayesian statistics, and methods for survey analysis.\n","date":1586304000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1586364432,"objectID":"a99ca1d34add3871e4b8e2225c007a06","permalink":"/authors/bailey-fosdick/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/bailey-fosdick/","section":"authors","summary":"Bailey Fosdick is an assistant professor of statistics at Colorado State University. Her primary research interests lie in the development of statistical methods for analyzing network data, with particular attention to applications in ecology and the social sciences. She also studies covariance models for multiway data, Bayesian statistics, and methods for survey analysis.","tags":null,"title":"Bailey Fosdick","type":"authors"},{"authors":null,"categories":null,"content":"Mikaela Elder is a undergraudate student in the Department of Biochemistry and Molecular Biology at Colorado State University. She has worked in a genetics lab on a project investigating the effects of mutation in proteins linked to neurodegenerative diseases and a project investigating the atypical structural tendencies among low-complexity domains in the Protein Data Bank proteome. Her goals are to learn how to mathematically model biological systems in an effort to better understand the mechanisms of biochemical processes.\n","date":1586131200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1586211407,"objectID":"2c23a0a0af5e97a1815648da4252c925","permalink":"/authors/mikaela-elder/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mikaela-elder/","section":"authors","summary":"Mikaela Elder is a undergraudate student in the Department of Biochemistry and Molecular Biology at Colorado State University. She has worked in a genetics lab on a project investigating the effects of mutation in proteins linked to neurodegenerative diseases and a project investigating the atypical structural tendencies among low-complexity domains in the Protein Data Bank proteome. Her goals are to learn how to mathematically model biological systems in an effort to better understand the mechanisms of biochemical processes.","tags":null,"title":"Mikaela Elder","type":"authors"},{"authors":null,"categories":null,"content":"Zach Laubach\u0026rsquo;s research is grounded in behavioral ecology and evolutionary biology. In particular, he tries to understand the ways in which early life environments shape phenotype. He is drawn to questions that explore the interrelations among social behaviors, molecular mechanisms, and stress physiology. He uses tools from diverse fields, including molecular biology and physiology to identify proximate mechanisms of animal behaviors and phenotypes; and causal inference methods from epidemiology to better understand relationships gleaned from observational data. He has carried out his research in birds, hyenas, and humans.\n","date":1585526400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1585625857,"objectID":"995dbc8c49ceaac8348f9682a5aeed3c","permalink":"/authors/zach_laubach/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zach_laubach/","section":"authors","summary":"Zach Laubach\u0026rsquo;s research is grounded in behavioral ecology and evolutionary biology. In particular, he tries to understand the ways in which early life environments shape phenotype. He is drawn to questions that explore the interrelations among social behaviors, molecular mechanisms, and stress physiology. He uses tools from diverse fields, including molecular biology and physiology to identify proximate mechanisms of animal behaviors and phenotypes; and causal inference methods from epidemiology to better understand relationships gleaned from observational data.","tags":null,"title":"Zach Laubach","type":"authors"},{"authors":null,"categories":null,"content":"Brooke Anderson is an assistant professor of environmental epidemiology at Colorado State University. Her research focuses on the health risks associated with climate-related exposures, including heat waves and air pollution, for which she has conducted several national-level studies. As part of her research, she has also published a number of open source R software packages to facilitate environmental epidemiologic research.\n","date":1583971200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1584040643,"objectID":"9eeff3b66d9db9aba8ad7e1cd7ebf977","permalink":"/authors/brooke-anderson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/brooke-anderson/","section":"authors","summary":"Brooke Anderson is an assistant professor of environmental epidemiology at Colorado State University. Her research focuses on the health risks associated with climate-related exposures, including heat waves and air pollution, for which she has conducted several national-level studies. As part of her research, she has also published a number of open source R software packages to facilitate environmental epidemiologic research.","tags":null,"title":"Brooke Anderson","type":"authors"},{"authors":null,"categories":null,"content":"Daniel Dean is a graduate student in the Department of Agriculture and Biology at Colorado State University. He is studying the soil microbiome and interested in learning additional skills in R programming and statistics.\n","date":1583452800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1583452800,"objectID":"05bf39477bf7e4b59f2e5aed40d43fa8","permalink":"/authors/daniel-dean/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/daniel-dean/","section":"authors","summary":"Daniel Dean is a graduate student in the Department of Agriculture and Biology at Colorado State University. He is studying the soil microbiome and interested in learning additional skills in R programming and statistics.","tags":null,"title":"Daniel Dean","type":"authors"},{"authors":null,"categories":null,"content":"Sarah Cooper is a graduate student in the Department of Microbiology, Immunology, Pathology at Colorado State University.\n","date":1582502400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1582502400,"objectID":"a2f4c4d8a79a270a1ff69d28937404db","permalink":"/authors/sarah-cooper/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/sarah-cooper/","section":"authors","summary":"Sarah Cooper is a graduate student in the Department of Microbiology, Immunology, Pathology at Colorado State University.","tags":null,"title":"Sarah Cooper","type":"authors"},{"authors":null,"categories":null,"content":"Burton Karger is a Research Associate in the Department of Microbiology Immunology and Pathology at Colorado State University.\n","date":1582243200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1582321282,"objectID":"7979d3030119d29653a50463c423f481","permalink":"/authors/burton-karger/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/burton-karger/","section":"authors","summary":"Burton Karger is a Research Associate in the Department of Microbiology Immunology and Pathology at Colorado State University.","tags":null,"title":"Burton Karger","type":"authors"},{"authors":null,"categories":null,"content":"Amy Fox is a graduate student in the Department of Microbiology, Immunology, and Pathology at Colorado State University. She\u0026rsquo;s currently testing the efficacy of different tuberculosis vaccines in mice and working on developing an R-based data analysis pipeline for flow cytometry data. When she\u0026rsquo;s not in the lab, she enjoys traveling and experiencing new food and cultures.\n","date":1582156800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1582156800,"objectID":"7208808b98af56edaf6697e234c3877b","permalink":"/authors/amy-fox/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/amy-fox/","section":"authors","summary":"Amy Fox is a graduate student in the Department of Microbiology, Immunology, and Pathology at Colorado State University. She\u0026rsquo;s currently testing the efficacy of different tuberculosis vaccines in mice and working on developing an R-based data analysis pipeline for flow cytometry data. When she\u0026rsquo;s not in the lab, she enjoys traveling and experiencing new food and cultures.","tags":null,"title":"Amy Fox","type":"authors"},{"authors":null,"categories":null,"content":"Sere Williams is a graduate student in the Department of Cellular and Molecular Biology at Colorado State University. She recently completed her MSc studying gene expression in rice exposed to drought stress. She\u0026rsquo;s excited to begin her PhD. This year she is rotating through labs working on immune response in tobacco, modeling hormone interactions in Arabidopsis, frost tolerance in weeds, and electron transport in Archaea.\n","date":1582070400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1582151148,"objectID":"359ce304229cff7b3cd9e06860d9e8f8","permalink":"/authors/sere-williams/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/sere-williams/","section":"authors","summary":"Sere Williams is a graduate student in the Department of Cellular and Molecular Biology at Colorado State University. She recently completed her MSc studying gene expression in rice exposed to drought stress. She\u0026rsquo;s excited to begin her PhD. This year she is rotating through labs working on immune response in tobacco, modeling hormone interactions in Arabidopsis, frost tolerance in weeds, and electron transport in Archaea.","tags":null,"title":"Sere Williams","type":"authors"},{"authors":null,"categories":null,"content":"Sierra Pugh is a graduate student in the Department of Statistics at Colorado State University. She has interest in Bayesian statistics, and has experience in spatial statistics.\n","date":1581552000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1581691815,"objectID":"a0b6f7b6e4c0c6781dd1b5bb9f45c4be","permalink":"/authors/sierra-pugh/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/sierra-pugh/","section":"authors","summary":"Sierra Pugh is a graduate student in the Department of Statistics at Colorado State University. She has interest in Bayesian statistics, and has experience in spatial statistics.","tags":null,"title":"Sierra Pugh","type":"authors"},{"authors":null,"categories":null,"content":"Camron Pearce is a graduate student in the Department of Microbiology at Colorado State University. His research includes researching drug therapies against tuberculosis and determining particle localization using confocal microscopy. His personal life includes marathon training, skiing on the weekend, and finding the next best fishing hole.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"d7248a08ab17e3d5c2bea65700d68a9b","permalink":"/authors/camron-pearce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/camron-pearce/","section":"authors","summary":"Camron Pearce is a graduate student in the Department of Microbiology at Colorado State University. His research includes researching drug therapies against tuberculosis and determining particle localization using confocal microscopy. His personal life includes marathon training, skiing on the weekend, and finding the next best fishing hole.","tags":null,"title":"Camron Pearce","type":"authors"},{"authors":null,"categories":null,"content":"James DiLisio is a graduate student in the Department of Microbiology, Immunology, and Pathology at Colorado State University. He is interested in modulating immune cell phentoypes in various disease states to improve current therapies.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"84c043c7c8c0042274f75edaccfe9957","permalink":"/authors/james_dilisio/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/james_dilisio/","section":"authors","summary":"James DiLisio is a graduate student in the Department of Microbiology, Immunology, and Pathology at Colorado State University. He is interested in modulating immune cell phentoypes in various disease states to improve current therapies.","tags":null,"title":"James DiLisio","type":"authors"},{"authors":null,"categories":null,"content":"Mike Lyons is an assistant professor at Colorado State University. He works on the development and application of mathematical and computational tools for tuberculosis clinical trials. This work involves both conventional pharmacokinetic/pharmacodynamic modeling and simulation as well as physiological modeling and the use of engineering-based approaches to design optimized combination drug regimens.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2f8f117d42fb07fcd0da9af3c5821003","permalink":"/authors/mike-lyons/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mike-lyons/","section":"authors","summary":"Mike Lyons is an assistant professor at Colorado State University. He works on the development and application of mathematical and computational tools for tuberculosis clinical trials. This work involves both conventional pharmacokinetic/pharmacodynamic modeling and simulation as well as physiological modeling and the use of engineering-based approaches to design optimized combination drug regimens.","tags":null,"title":"Mike Lyons","type":"authors"},{"authors":null,"categories":null,"content":"Sherry WeMott-Colton is a graduate student in the Department of Environmental and Radiological Health Sciences at Colorado State University. Her research focuses on evironmental and social factors impacting health.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"93e55912b52bf4b4c2aca0826be7d218","permalink":"/authors/sherry-wemott/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/sherry-wemott/","section":"authors","summary":"Sherry WeMott-Colton is a graduate student in the Department of Environmental and Radiological Health Sciences at Colorado State University. Her research focuses on evironmental and social factors impacting health.","tags":null,"title":"Sherry WeMott","type":"authors"},{"authors":["Bailey Fosdick"],"categories":["class meeting"],"content":"\rLinks for April 9\rLarge group: meeting link\n\rGroup 1 (Sierra, Daniel, Burton): meeting link\rGroup 2 (Camron, Sarah, Mikaela): meeting link\rGroup 3 (Sere, James, Sherry, Amy): meeting link\r\r\rAdditional links\r\rDESeq2 vignette\redgeR vignette\r\r\rVocabulary quiz for April 9\rLoading…\r\r\r","date":1586304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586364432,"objectID":"3aee6c58c71f4e9cbdf1b625d5ec25ad","permalink":"/post/details-for-class-on-april-9/","publishdate":"2020-04-08T00:00:00Z","relpermalink":"/post/details-for-class-on-april-9/","section":"post","summary":"\rLinks for April 9\rLarge group: meeting link\n\rGroup 1 (Sierra, Daniel, Burton): meeting link\rGroup 2 (Camron, Sarah, Mikaela): meeting link\rGroup 3 (Sere, James, Sherry, Amy): meeting link\r\r\rAdditional links\r\rDESeq2 vignette\redgeR vignette\r\r\rVocabulary quiz for April 9\rLoading…\r\r\r","tags":["class meeting"],"title":"Details for class on April 9","type":"post"},{"authors":["Mikaela Elder"],"categories":["vocabulary","chapter 8"],"content":"\r\rChapter 8 covers high-throughput count data, like data generated through RNA-seq. It introduces a number of tools that are useful for analyzing this type of data. The vocabulary terms for Chapter 8 are:\n\r\r\r\r\r\r\r\rRNA-Seq\r\rsequencing of RNA molecules found in a population of cells or in a tissue\r\r\r\rChIP-Seq\r\rsequencing of DNA regions that are bound to particular DNA-binding proteins (selected by immunoprecipitation)\r\r\r\rRIP-Seq\r\rsequencing of RNA molecules, or regions of them, bound to a particular RNA-binding protein\r\r\r\rDNA-Seq\r\rsequencing of genomic DNA\r\r\r\rHiC\r\rhigh-throughput chromatin conformation capture; a technique that aims to map the 3D spatial arrangement of DNA\r\r\r\rcDNA\r\rcomplementary DNA made from RNA templates and reverse transcriptase; used in RNA-Seq\r\r\r\rgenetic screens\r\ra technique looking at the proliferation or survival of cells upon gene knockdown, knockout, or modification\r\r\r\rread\r\rthe sequence obtained from a fragment\r\r\r\rsequencing library\r\rthe collection of DNA molecules used as input for the sequencing machine\r\r\r\rfragments\r\rmolecules being sequenced during a sequencing analysis\r\r\r\rcount table\r\ra matrix with the tallies of the number of occurrences of subpopulations from a larger population/sample\r\r\r\rdynamic range\r\ra ratio between the maximum and minimum values\r\r\r\rheteroskedasticity\r\ra phenomenon where the variance and distribution shape of the data in different parts of the dynamic range are very different\r\r\r\rnormalization\r\ra technique that adjusts for the nature and magnitude of systematic sampling biases\r\r\r\rrare events\r\roccurrences in the tail(s) of a distribution; observations that are extraordinarily high or low\r\r\r\rdispersion\r\ra measure of the spread of the data; a common measure is the standard deviation or variance\r\r\r\rgamma-Poisson\r\rnegative binomial distribution with 2 parameters; 𝛼 and 𝛽\r\r\r\rsystematic biases\r\rsystematic distortions that affect the data generation and need to be accounted for in the analysis; one example would be variations in the total number of reads for each sample in a sequencing experiment\r\r\r\rmetadata\r\ra set of data that describes or gives information about other data\r\r\r\rmultifactorial design\r\ran experimental design with more than one independent variable\r\r\r\rbalanced\r\rin the context of study design, these are where there is an equal number of observations of all combinations of factors being tested\r\r\r\rdifferential expression analysis\r\ra type of analysis that uses the normalized read count data to investigate quantitative changes in expression levels between different experimental groups\r\r\r\rintercept\r\ra coefficient representing the base level of the measurement in the negative control\r\r\r\rdesign factors\r\rbinary indicator variables\r\r\r\rinteraction effect\r\ra parameter in a model that accounts for the effects of two experimental factors that combine in a more complicated fashion than a simple summation\r\r\r\rdesign matrix\r\ra matrix encoding the design of an experiment where the columns correspond to experimental factors and the rows correspond to different experimental conditions\r\r\r\rresiduals\r\ra term in a model that reflects the experimental fluctuations (i.e. random noise)\r\r\r\rleast sum-of-squares fitting\r\ra type of model fitting that minimizes the sum of the squared residuals\r\r\r\rlinear model\r\ra model that is a linear function of parameters, i.e. takes the form: y_j = sum_k (x_jk * beta_k + e_j)\r\r\r\ranalysis of variance (ANOVA)\r\ran analysis that decomposes patterns in the data into systematic variability and noise\r\r\r\rnoise\r\rvariability unaccounted for by model parameters\r\r\r\rsystematic variability\r\rvariability accounted for by model parameters\r\r\r\rbreakdown point\r\ra measure of the robustness of an estimator; larger values indicate more robust estimators\r\r\r\rrobust\r\ra “sturdy” estimator that is not heavily influenced by outliers\r\r\r\rleast absolute deviations\r\rminimization of the sum of the absolute values of the residuals\r\r\r\rleast quantile of squares\r\ra type of regression where the difference between the model quantile and empirical quantile is minimized\r\r\r\rleast trimmed sum of squares\r\ra type of regression that minimized the sum of squared residuals, where the sum is over only a fraction of the smallest residuals\r\r\r\rlogistic regression\r\ra type of generalized linear regression for binary data where the outcome is transformed by the logistic function and bounded between 0 and 1\r\r\r\rmaximum likelihood\r\ra method for parameter estimation that finds the parameter value that maximizes the probablity of the observed data under the model\r\r\r\rlikelihood\r\ra function of a model parameter which is equal to the probability of the observed data under the model\r\r\r\rmaximum-likelihood estimates\r\rmodel parameters that are estimated by maximizing the probability of the observed data under the model\r\r\r\rnuisance factor / blocking factor\r\ra factor that has some effect on the response but is of no interest to the experiment\r\r\r\rbatch effects\r\rhidden factors that affect the data but are not documented; e.g. running samples at the same time have a degree of similarity from being run in the same batch\r\r\r\rpseudocounts\r\rtransformations that take the form y = log2(n + n_0) where n is the count and n_0 is a chosen positive constant\r\r\r\rvariance stabilizing transformation\r\ra transformation that has finite values and finite slope, even for counts close to zero\r\r\r\rregularized logarithm (rlog) transformation\r\ra technique that transforms the original count data to a log2-like scale by fitting a “trivial” model with a separate term for each sample and a prior distribution on the coefficients which is estimated from the data\r\r\r\rCook’s distance\r\ra measure of how much a single sample is influencing the coefficients in a model; large values indicate an outlier count\r\r\r\rsampling without replacement\r\ra random sample in which no observation occurs more than one time in the sample\r\r\r\rnull hypothesis\r\roften, a hypothesis of “no association” that is used as a counterpart to a more interesting alternative hypothesis in hypothesis testing.\r\r\r\rvariability\r\rin statistics, the amount by which a set of observations deviate from their mean\r\r\r\routlier\r\ra data point that does not follow the pattern of the rest of the data; often this data point will have a large residual\r\r\r\rM-estimation\r\ra type of regression analysis that is more robust than OLS to outliers or data that does not follow a normal distribution; it minimizes the sum of the penalization function applied to the residuals\r\r\r\rconservative\r\ran approach that prioritizes reducing false positives\r\r\r\rsplicing\r\ra process in eukaryotic organisms where mRNA is cut down from the full-length gene to just the exons before being translated\r\r\r\rexons\r\rsegments of a gene that actually get used during translation or encode for a protein\r\r\r\risoforms\r\rdifferent forms of the same gene that result from splicing events that combine different exons in an mRNA script\r\r\r\rupregulated\r\ra term used to describe the increased expression of a gene\r\r\r\rgene knockdown\r\ra way of inactivating a gene by targeting its mRNA transcript for inactivation or degradation\r\r\r\rgene knockout\r\rdeletion of a gene from the genome\r\r\r\rtranscriptome\r\rthe total of all of the mRNA expressed from genes in an organism\r\r\r\rpolymorphism\r\rgenetic variation within a population\r\r\r\r\rSources consulted or cited\rSome of the definitions above are based in part or whole on listed definitions in the following sources.\n\rHolmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press,\rCambridge, United Kingdom.\rLexico: https://www.lexico.com\rStatistics How To: https://www.statisticshowto.com\rLavrakas, 2008. Sampling without replacement. Encyclopedia of Survey Research Methods. https://dx.doi.org/10.4135/9781412963947.n516\r\r\rPractice\r\r\r","date":1586131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586211407,"objectID":"faf087f699e0eef13cfb4f9ef67d569b","permalink":"/post/chapter-8-vocabulary-list/","publishdate":"2020-04-06T00:00:00Z","relpermalink":"/post/chapter-8-vocabulary-list/","section":"post","summary":"Chapter 8 covers high-throughput count data, like data generated through RNA-seq. It introduces a number of tools that are useful for analyzing this type of data. The vocabulary terms for Chapter 8 are:\n\r\r\r\r\r\r\r\rRNA-Seq\r\rsequencing of RNA molecules found in a population of cells or in a tissue\r\r\r\rChIP-Seq\r\rsequencing of DNA regions that are bound to particular DNA-binding proteins (selected by immunoprecipitation)\r\r\r\rRIP-Seq\r\rsequencing of RNA molecules, or regions of them, bound to a particular RNA-binding protein\r\r\r\rDNA-Seq\r\rsequencing of genomic DNA\r\r\r\rHiC\r\rhigh-throughput chromatin conformation capture; a technique that aims to map the 3D spatial arrangement of DNA\r\r\r\rcDNA\r\rcomplementary DNA made from RNA templates and reverse transcriptase; used in RNA-Seq\r\r\r\rgenetic screens\r\ra technique looking at the proliferation or survival of cells upon gene knockdown, knockout, or modification\r\r\r\rread\r\rthe sequence obtained from a fragment\r\r\r\rsequencing library\r\rthe collection of DNA molecules used as input for the sequencing machine\r\r\r\rfragments\r\rmolecules being sequenced during a sequencing analysis\r\r\r\rcount table\r\ra matrix with the tallies of the number of occurrences of subpopulations from a larger population/sample\r\r\r\rdynamic range\r\ra ratio between the maximum and minimum values\r\r\r\rheteroskedasticity\r\ra phenomenon where the variance and distribution shape of the data in different parts of the dynamic range are very different\r\r\r\rnormalization\r\ra technique that adjusts for the nature and magnitude of systematic sampling biases\r\r\r\rrare events\r\roccurrences in the tail(s) of a distribution; observations that are extraordinarily high or low\r\r\r\rdispersion\r\ra measure of the spread of the data; a common measure is the standard deviation or variance\r\r\r\rgamma-Poisson\r\rnegative binomial distribution with 2 parameters; 𝛼 and 𝛽\r\r\r\rsystematic biases\r\rsystematic distortions that affect the data generation and need to be accounted for in the analysis; one example would be variations in the total number of reads for each sample in a sequencing experiment\r\r\r\rmetadata\r\ra set of data that describes or gives information about other data\r\r\r\rmultifactorial design\r\ran experimental design with more than one independent variable\r\r\r\rbalanced\r\rin the context of study design, these are where there is an equal number of observations of all combinations of factors being tested\r\r\r\rdifferential expression analysis\r\ra type of analysis that uses the normalized read count data to investigate quantitative changes in expression levels between different experimental groups\r\r\r\rintercept\r\ra coefficient representing the base level of the measurement in the negative control\r\r\r\rdesign factors\r\rbinary indicator variables\r\r\r\rinteraction effect\r\ra parameter in a model that accounts for the effects of two experimental factors that combine in a more complicated fashion than a simple summation\r\r\r\rdesign matrix\r\ra matrix encoding the design of an experiment where the columns correspond to experimental factors and the rows correspond to different experimental conditions\r\r\r\rresiduals\r\ra term in a model that reflects the experimental fluctuations (i.","tags":["vocabulary","chapter 8"],"title":"Chapter 8 Vocabulary List","type":"post"},{"authors":["Bailey Fosdick"],"categories":["class meeting"],"content":"\rGeneral course information\rWe are going to try meeting each week using Microsoft Teams during our usual time on Thursdays, 3-5pm. The class structure will remain the same as before, however everything will be online:\n3:00-3:10 pm: Take the quiz through the online blog post.\r3:10-4:00 pm: Large group meeting for group discussion.\r4:00-5:00 pm: Smaller meetings for groups to work on the chapter exercise.\n\rLinks for April 2\rLarge group: meeting link\n\rGroup 1 (Sierra, Camron, James): meeting link\rGroup 2 (Sherry, Amy, Mikaela): meeting link\rGroup 3 (Daniel, Burton, Sarah, Sere): meeting link\r\r\rVocabulary quiz for April 2\rLoading…\r\r\r","date":1585612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585665132,"objectID":"a3086d5472c57c6fd375723b87696627","permalink":"/post/details-for-class-on-april-2/","publishdate":"2020-03-31T00:00:00Z","relpermalink":"/post/details-for-class-on-april-2/","section":"post","summary":"General course information\rWe are going to try meeting each week using Microsoft Teams during our usual time on Thursdays, 3-5pm. The class structure will remain the same as before, however everything will be online:\n3:00-3:10 pm: Take the quiz through the online blog post.\r3:10-4:00 pm: Large group meeting for group discussion.\r4:00-5:00 pm: Smaller meetings for groups to work on the chapter exercise.\n\rLinks for April 2\rLarge group: meeting link","tags":["class meeting"],"title":"Details for class on April 2","type":"post"},{"authors":["Zach Laubach"],"categories":["Chapter 7","vocabulary"],"content":"\r\rChapter 7 covers multivariate analysis, with a focus on principal component analysis and\rdimension reduction in general.\n\r\r\r\r\r\r\r\rprincipal component analysis (PCA)\r\ran unsupervised ordination method used to reduce the dimensionality of data by creating scores that maximize the explained variation in the data\r\r\r\rmatrix\r\ra two dimensional arrangement of rows and columns used to store data\r\r\r\rmass spectroscopy\r\ra measurement procedure based on the mass-to-charge ratio of ions, often used to measure metabolite abundance\r\r\r\rcorrelation coefficient\r\ra measure of how two variables co-vary, reported as a single summary value\r\r\r\rcentering\r\rsubtracting the mean of the data so the new mean is 0\r\r\r\rscaling / standardizing\r\rdividing data values by the data’s standard deviation so the new standard deviation is 1\r\r\r\rdata simplification\r\ra broadly applicable term referring to the process of summarizing or reducing the dimensions of multivariate data\r\r\r\rdimension reduction\r\rsummarizing data to reduce the number of variables for downstream analyses\r\r\r\rprincipal scores\r\ra normally distributed z-score assigned to each subject that corresponds with the specific ordering and weighting of original variables within a given principal component\r\r\r\runsupervised learning\r\ra machine learning method used to find patterns in the data without a priori variable ranking or labeling\r\r\r\rstatus\r\rin the context of variables in a statistical learning algorithm, a ranking or labeling of variables (e.g., to consider one variable as the outcome or goal and the rest as potential predictive variables)\r\r\r\rprojection\r\ra representation of data from a higher dimensional space to a lower dimensional space\r\r\r\rlinear\r\rin the context of a statistical technique, a description that describes the search for relationships between variables that can be expressed as a linear combination of predictors\r\r\r\rregression line\r\ra linear function of the form y = mx + b which is used to project two-dimensional data onto a 1 dimensional line\r\r\r\rlinear regression\r\ra supervised method that models the relationship between explanatory and response variables by minimizing the residual sum of squares with respect to the response variable\r\r\r\rsupervised learning\r\rin the context of a statistical learning technique, a machine learning method that uses specified, user defined inputs to map patterns (input/output associations) in data\r\r\r\rpredictor\r\ran independent, explanatory, or ‘x’ variable in a model\r\r\r\rresponse\r\ran outcome or ‘y’ variable in a model that is thought to be affected by a predictor\r\r\r\rprincipal components\r\runcorrelated latent variables created by the PCA procedure, of which there are as many as there are original variables entered into the procedure\r\r\r\rinertia\r\rin the context of variability of points, the total variance of a point cloud based on the sum of squares of the projection of points\r\r\r\rlinear combination\r\rmathematical expression in which terms are scaled by constants and then added together\r\r\r\rloadings\r\rin the context of principal components, these values quantify the weight of each original variable in a principal component\r\r\r\rsingular value decomposition (SVD)\r\ra way to decompose a rectangular matrix by factoring it into three different matrices in a way that has some useful mathematical applications\r\r\r\rrank\r\rin the context of a matrix, the maximum number of linearly independent column or row vectors\r\r\r\rnorm\r\rin the context of a vector, a positive scalar quantity reflecting its size/magnitude\r\r\r\rsingular value\r\ra non-negative, normalizing value from a singular value decomposition quantifying the relative importance of the corresponding singular vectors\r\r\r\rorthonormal\r\rthe characteristic of a set of vectors that are both orthogonal (uncorrelated) and normalized\r\r\r\rprincipal plane\r\ra 2-dimensional space across which the data are most spread out or variable\r\r\r\rtrace\r\rin the context of matrices, the sum of the diagonal elements of a square matrix\r\r\r\rsupplementary information\r\rextra information or instruction to help clarify research question, procedure or results\r\r\r\rmetadata\r\rinformation, data, or descriptions that characterize other data\r\r\r\rbiplot\r\ra type of exploratory graph that displays information on both the observations and the variables of a data matrix\r\r\r\rbiometric characteristics\r\rphysical, physiological, demographic, or behavioral features of an organism that can be measured and quantified\r\r\r\rproliferation rate\r\rspeed at which the number of cells increase through the process of cellular division\r\r\r\rgene expression profile\r\ra snapshot measure of the level of activity/expression (transcription) of a collection (thousands) of genes, representing a global measure of gene function\r\r\r\rT-cell populations\r\rgroups of differentiated white blood cells that function in immune response\r\r\r\roperational taxonomic units (OTUs)\r\rclusters of closely related species of bacteria based on sequence similarity\r\r\r\rtranscriptome data\r\rthe complete set of all RNA molecules measured from a biological sample generated from genome-wide sequencing methods, like RNA-seq\r\r\r\rsequence read\r\ran inferred sequence of base pairs, or fragments of the genome, generated from one of many genomics methods\r\r\r\rproteomic profile\r\ra snapshot measure of the levels of all proteins measured in a biological sample\r\r\r\rmolecule\r\rtwo or more chemically bond atoms that lack a charge\r\r\r\rm/z ratio\r\rmass to charge ratio used in mass spectrometry to differentiation molecules\r\r\r\rwild-type\r\ra normal allele or phenotype that occurs under natural conditions\r\r\r\r\rSource Consulted or Cited\rSome of the definitons above are based in part or whole on listed definitions in the following source:\n\rHolmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press,\rCambridge, United Kingdom.\rWikipedia: The Free Encyclopedia. http://en.wikipedia.org/wiki/Main_Page\r\r\rPractice\r\r\r","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585625857,"objectID":"599af682b514a006f13ca6642d76980a","permalink":"/post/vocabulary-for-chapter-7/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-7/","section":"post","summary":"Chapter 7 covers multivariate analysis, with a focus on principal component analysis and\rdimension reduction in general.\n\r\r\r\r\r\r\r\rprincipal component analysis (PCA)\r\ran unsupervised ordination method used to reduce the dimensionality of data by creating scores that maximize the explained variation in the data\r\r\r\rmatrix\r\ra two dimensional arrangement of rows and columns used to store data\r\r\r\rmass spectroscopy\r\ra measurement procedure based on the mass-to-charge ratio of ions, often used to measure metabolite abundance\r\r\r\rcorrelation coefficient\r\ra measure of how two variables co-vary, reported as a single summary value\r\r\r\rcentering\r\rsubtracting the mean of the data so the new mean is 0\r\r\r\rscaling / standardizing\r\rdividing data values by the data’s standard deviation so the new standard deviation is 1\r\r\r\rdata simplification\r\ra broadly applicable term referring to the process of summarizing or reducing the dimensions of multivariate data\r\r\r\rdimension reduction\r\rsummarizing data to reduce the number of variables for downstream analyses\r\r\r\rprincipal scores\r\ra normally distributed z-score assigned to each subject that corresponds with the specific ordering and weighting of original variables within a given principal component\r\r\r\runsupervised learning\r\ra machine learning method used to find patterns in the data without a priori variable ranking or labeling\r\r\r\rstatus\r\rin the context of variables in a statistical learning algorithm, a ranking or labeling of variables (e.","tags":["Chapter 7","vocabulary"],"title":"Vocabulary for Chapter 7","type":"post"},{"authors":["Brooke Anderson"],"categories":["Chapter 6","quiz"],"content":"\rThe vocabulary quiz will be live here during the start of the course.\nLoading…\r\r","date":1583971200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584040643,"objectID":"4bf4bf373f3cd7c717411b606027c93c","permalink":"/post/chapter-6-vocabular-zuiz/","publishdate":"2020-03-12T00:00:00Z","relpermalink":"/post/chapter-6-vocabular-zuiz/","section":"post","summary":"\rThe vocabulary quiz will be live here during the start of the course.\nLoading…\r\r","tags":["Chapter 6","quiz"],"title":"Chapter 6 vocabulary quiz","type":"post"},{"authors":["Daniel Dean"],"categories":["Chapter 6","vocabulary"],"content":"\r\rChapter 6 covers Statistical Testing, including a review of null and alternative hypotheses (and associated distributions), types of error (I and II), as well as challenges and opportunities introduced by multiple testing.\n\r\r\r\r\r\r\r\rOccam’s razor\r\rHeuristic stating that the simplest explanation for a phenomenon is often the best\r\r\r\rrejection region\r\rSubset of possible outcomes for which probabilities under the null hypothesis fall under a low probability threshold, e.g. outcomes with a null-distribution probability \u0026lt; 0.05; if an outcome falls within this region (e.g., p \u0026lt; 0.05), it suggests that the null hypothesis is not true.\r\r\r\rtest statistic\r\rMetric for measuring how well a null hypothesis fits the data\r\r\r\rnull hypothesis\r\rHypothesis describing some ‘uninteresting’ outcome (e.g., that no difference exists between certain groups of events/outcomes)\r\r\r\rnull distribution\r\rDistribution of possible outcomes, given that the null hypothesis is true\r\r\r\ralternative hypothesis\r\rA hypothesis providing a different probability distribution than the null hypothesis; conceptually, holds that some difference from the null hypothesis exists (e.g. different means, frequencies, trends)\r\r\r\rsignificance level/false positive rate/Type I error\r\rProbability of incorrectly rejecting the null hypothesis due to outcomes falling within the rejection region by chance; in terms of the null distribution, total probability that the outcome could fall within the rejection region given that the null hypothesis is true.\r\r\r\rpower\r\rTrue positive rate of a test (i.e., probability that an outcome falls in the rejection region of the null distribution, given that the alternative hypothesis is true)\r\r\r\rfalse negative rate/Type II error\r\rProbability of incorrectly failing to reject the null hypothesis when an outcome from the alternative hypothesis distribution fails to fall within the rejection region of the null hypothesis.\r\r\r\rspecificity\r\rComplement of false positive rate (Type I error); probability of a test failing to reject the null hypothesis when it is true.\r\r\r\rpower/sensitivity/true negative rate\r\rComplement of false negative rate (Type II error); probability of correctly rejecting null hypothesis if the alternative hypothesis is true.\r\r\r\rassumption of independence\r\rTreating every observation in a dataset as if it has no influence on the outcomes of other observations (or at least none unaccounted-for in the model).\r\r\r\rp-value hacking\r\rFallaciously ‘fishing’ for significant results by running tests until a small p-value is obtained by chance; this can be deliberate or inadvertently caused by a scattershot approach to testing.\r\r\r\rhypothesis switching\r\rFallacy of generating and/or changing hypotheses for a set of known results until a significant result is obtained by chance.\r\r\r\rfamily-wide error rate (FWER)\r\rProbability of at least one false positive occurring in repeated tests. Assuming independent tests, this is the complement of the probability of only true positives occurring, and approaches 1.0 as the number of tests approaches infinity.\r\r\r\rp-value histogram\r\rVisualization to get a quick sense of p-value distribution of possible test outcomes for a null hypothesis. The distribution is a mixture of cases where the null hypothesis is rejected (small p-values) or retained (larger p-values).\r\r\r\rfalse discovery rate (FDR)\r\rThe proportion of false positives among all cases where the null hypothesis is rejected across an entire distribution.\r\r\r\rlocal false discovery rate (fdr)\r\rThe probability of Type I Error at a given p-value when the distribution of the p-values is treated as a mixture model of the null distribution and alternative hypothesis distribution. This varies based on the p-value, rather than being a property of the entire distribution.\r\r\r\rtail-area false disovery rate (Fdr)\r\rIntegration-based extension of the local false discovery rate to obtain a false discovery rate for the entire distribution.\r\r\r\rindependent filtering\r\rMethod to increase test power by filtering variables with criteria that are independent under the null hypothesis, but correlated under the alternative\r\r\r\rindependent hypothesis weighting\r\rA method of improving power of multiple testing by weighting hypotheses based on their power\r\r\r\rBonferroni adjustment\r\rMethod used to compensate for inflated Type I (false positive) error in multiple testing by dividing the test significance level/hypothesis threshold (e.g., alpha = 0.05) by the number of tests performed\r\r\r\rwhole genome sequencing\r\rMethod used to determine and record the DNA base values and order across all of an organism’s genes\r\r\r\rmarker gene\r\rA gene used to determine membership in a group of interest (e.g., a taxon, genotype within a population, or possessing a certain metabolic trait)\r\r\r\rexpression level\r\rThe realtive abundance of transcriptions of a gene of interest present in, e.g., a cell or environment\r\r\r\rreagent\r\ra compound used in creating a chemical reaction like an assay\r\r\r\rhypothesis testing\r\rEvaluating whether outcomes are sufficently unlikely under the null hypothesis (holding that outcomes are determined fully by chance) that it can be rejected in favor of an alternative hypothesis\r\r\r\rworkflow\r\rA sequence of steps used in carrying out a larger operation or process\r\r\r\rtwo-sided test\r\rA statistical test which rejects the null hypothesis if an observed test statistic is either too large or too small compared to that expected under the null hypothesis\r\r\r\rone-sided test\r\rA statistical test which rejects the null hypothesis if an observed test statistic departs from the expected range in a single, predetermined direction (i.e. larger or smaller)\r\r\r\rtwo-sample\r\rIn the context of statistical testing, a situation whether the data belong to two known groups.\r\r\r\runpaired\r\rIn the context of statistical tests, these are used when comparing groups with independent measurements (e.g. the observations for one group have no association with observations from the other group)\r\r\r\requal variances\r\rWhen groups being compared have (substantially) equivalent levels of variability.\r\r\r\rdependence\r\rWhen the outcomes of two variables are associated with one another.\r\r\r\rexpected value\r\rFor a random quantity, this is the value of the mean, i.e. “average value”.\r\r\r\r\rSources Consulted or Cited\rSome of the definitons above are based in part or whole on listed definitions in the following sources:\n\rHolmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press,\rCambridge, United Kingdom.\rWikipedia: The Free Encyclopedia. http://en.wikipedia.org/wiki/Main_Page\rBourgon, R., Gentleman, R. \u0026amp; Huber, W. Independent filtering increases detection power for high-throughput experiments. Proceedings of the National Academy of Sciences 107, 9546–9551 (2010).\rIgnatiadis, N., Klaus, B., Zaugg, J. et al. Data-driven hypothesis weighting increases detection power in genome-scale multiple testing. Nat Methods 13, 577–580 (2016). https://doi.org/10.1038/nmeth.3885\rhttps://www.statisticssolutions.com/bonferroni-correction/\rhttps://bioconductor.org/packages/release/bioc/vignettes/IHW/inst/doc/introduction_to_ihw.html\rhttps://www.statisticshowto.datasciencecentral.com/familywise-error-rate/\rhttps://www.statisticshowto.datasciencecentral.com/probability-and-statistics/hypothesis-testing/\rhttps://www.biostars.org/p/273537/\nPractice\r\r\r\r","date":1583452800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583452800,"objectID":"daf9e88176b809bdbed22af1b1b580a4","permalink":"/post/vocabulary-for-chapter-6/","publishdate":"2020-03-06T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-6/","section":"post","summary":"Chapter 6 covers Statistical Testing, including a review of null and alternative hypotheses (and associated distributions), types of error (I and II), as well as challenges and opportunities introduced by multiple testing.\n\r\r\r\r\r\r\r\rOccam’s razor\r\rHeuristic stating that the simplest explanation for a phenomenon is often the best\r\r\r\rrejection region\r\rSubset of possible outcomes for which probabilities under the null hypothesis fall under a low probability threshold, e.","tags":["Chapter 6","vocabulary"],"title":"Vocabulary for Chapter 6","type":"post"},{"authors":["Brooke Anderson"],"categories":["Chapter 5","quiz"],"content":"\rThe vocabulary quiz will be live here during the start of the course.\nLoading…\r\r","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583445540,"objectID":"4ca9d7604bf3c82206fd30c69052f4e2","permalink":"/post/chapter-5-vocabulary-quiz/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/chapter-5-vocabulary-quiz/","section":"post","summary":"\rThe vocabulary quiz will be live here during the start of the course.\nLoading…\r\r","tags":["Chapter 5","quiz"],"title":"Chapter 5 vocabulary quiz","type":"post"},{"authors":["Brooke Anderson"],"categories":["quiz","Chapter 4"],"content":"\rThe vocabulary quiz will be live here during the start of the course.\nLoading…\r\r","date":1582761600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582825339,"objectID":"47162ba198ed97a383b0f7af77226d51","permalink":"/post/chapter-4-vocabulary-quiz/","publishdate":"2020-02-27T00:00:00Z","relpermalink":"/post/chapter-4-vocabulary-quiz/","section":"post","summary":"\rThe vocabulary quiz will be live here during the start of the course.\nLoading…\r\r","tags":["quiz","Chapter 4"],"title":"Chapter 4 vocabulary quiz","type":"post"},{"authors":["Burton Karger"],"categories":["Chapter 5","vocabulary"],"content":"\r\rChapter 5 covers Clustering Analysis for large scale data anlysis like DNA/RNA sequencing outputs. These methods produce so much data that more unbiased approaches are required when attempting to make correlations.\n\r\r\r\r\r\r\r\runsupervised method\r\rA learning method where all variables are treated with the same status, rather than one variable being considered as an outcome or target.\r\r\r\rstatus\r\rA variable’s classification as an outcome/predictor (e.g. independent/dependent) in an analysis.\r\r\r\rdistance\r\rA measure of the difference between two random variables.\r\r\r\rThe Euclidean distance\r\rA distance metric equal to the “ordinary” straight-line distance between two points.\r\r\r\rManhattan distance\r\rA distance metric equal to the sum of the absolute differences between the coordinate values for two points.\r\r\r\rMaximum distance\r\rA distance metric equal to the largest absolute difference between the coordinate values for two points.\r\r\r\rWeighted Euclidean distance\r\rA distance metric, which is a generalization of the ordinary Euclidean distance, that differentially weights the differences between the coordinate values for two points.\r\r\r\rMinkowski distance\r\rA distance metric equal to the mth root of the sum of the absolute differences between the coordinate values each raised to the mth power.\r\r\r\rEdit or Hamming distance\r\rA distance metric for comparing character sequences that counts the number of differences between two character strings.\r\r\r\rBinary distance\r\rA distance metric for binary strings based on the proportion of features having only one bit on amongst those features that have at least one bit on.\r\r\r\rJaccard distance\r\rA distance metric that quantifies how dissimilar two sets are.\r\r\r\rco-occurrence\r\rThe fact of two or more things occurring together or simultaneously.\r\r\r\rJaccard index\r\rA statistic used in quantifying the similarities between sample sets, which is formally defined as the size of the intersection between two sets divided by the size of the union of the sets.\r\r\r\rJaccard dissimilarity\r\r1 - the Jaccard index.\r\r\r\rCorrelation-based distance\r\rA distance metric that measures two objects to be similar if their features are highly correlated, even though the observed values may be far apart in terms of Euclidean distance.\r\r\r\rClusters of Differentiation (CDs)\r\rAt different stages of their development, immune cells express unique combinations of proteins on their surfaces.\r\r\r\rRectangular gating\r\rA method of identifying groups of cells from a flow cytometry experiment using either a line (one-dimensional) or the quadrants created by two perpendicular lines (two-dimensional)\r\r\r\rHyperbolic Arcsine (asinh)\r\rA transform function often preferred over the log tranform for flow cytometry data because it can be applied to negative values.\r\r\r\rdensity-based clustering (dbscan)\r\rThe dbscan method clusters points in dense regions according to the density-connectedness criterion. It looks at small neighborhood spheres to see if points are connected.\r\r\r\rcurse of dimensionality\r\rWhen the dimensionality increases, the volume of the space increases so fast that the available data become sparse\r\r\r\rdensity-reachability\r\rA fundamental criterion in dbscan that quantifies whether two points are close enough together and surrounded by sufficiently many other points.\r\r\r\rrecursive partitioning methods\r\rA class of methods for dividing heterogeneous populations into more homogeneous subgroups, often used to make decision trees, that starts by separating the whole population into a few groups and iteratively continues separating each into subgroups.\r\r\r\rminimal jump method/single linkage method/nearest neighbor method\r\rA clustering method that computes the distance between clusters as the smallest distance between any two points in the two clusters.\r\r\r\rmaximum jump method/complete linkage method\r\rA clustering method that defines the distance between clusters as the largest distance between any two objects in the two clusters.\r\r\r\raverage linkage method\r\rA clustering method that defines the distance between clusters as the average distance between a point in one cluster and another point in the other cluster.\r\r\r\rWard’s method\r\rA clustering method that takes an analysis of variance approach, where the goal is to minimize the variance within clusters. This method is very efficient, however, it tends to break the clusters up into ones of smaller sizes.\r\r\r\rWithin-groups sum of squares (WSS)\r\rA measure of the variability among data points within an identified cluster.\r\r\r\rCalinski-Harabasz index\r\rQuantifies the relative variability between groups (between group sum of squares) and within groups (within-groups sum of sqaures), similar to the F statistic used in analysis of variance.\r\r\r\rBetween-groups sum of squares (BSS)\r\rA measure of the variability between clusters.\r\r\r\rgap statistic\r\rA metric used to perform model selection which quantifies the amount of model fit improvement when using a more complex model. These can be used to select the number of clusters for a data set.\r\r\r\rtechnical / batch effects\r\rDepedence in data observations that results from technical differences between samples, such as the type of sequencing machine or the technician that ran the sample, rather than from scientifically interesting causes.\r\r\r\rcomputational complexity\r\rA measure of the computational resources needed to run an algorithm.\r\r\r\rnoise\r\rUnexplained variability within a data sample.\r\r\r\roperational taxonomic unit (OTU)\r\rA method of clustering organisms based on DNA sequence similarity of a certain taxonomic marker gene.\r\r\r\rbias\r\rThe tendency of a statistic to overestimate or underestimate a parameter.\r\r\r\rrepresentativeness heuristic\r\rA method of learning or discovery that assesses similarity of objects and organizes them based around a category prototype (e.g., like goes with like, and causes and effects should resemble each other).\r\r\r\rrare variants\r\rAn alternative form of a gene that occur just once or twice in an individual sample but more often across all samples.\r\r\r\rinsertion-deletion (indel)\r\rinsertion or deletion of bases in the genome of an organism.\r\r\r\rneighboring cluster\r\rThe cluster with the lowest average dissimilarity to a given cluster.\r\r\r\rsilhouette index\r\rA metric quantifying the degree to which a given data point belongs to its designated cluster.\r\r\r\rMicrobiome\r\rThe aggregate of all microbiota that reside on or within an organim’s tissues and biofluids along with the corresponding anatomical sites in which they reside.\r\r\r\rfiltering\r\rin the context of low-quality rRNA reads removal of low-quality reads and trimming them to a consistent length\r\r\r\rHistopathology\r\rThe microscopic examination of tissue in order to study the manifestations of disease.\r\r\r\rMolecular signature\r\rSets of genes, proteins, genetic variants or other variables that can be used as markers for a particular phenotype\r\r\r\rGene expression data\r\rGene expression measurements : from gene¬scale to genome¬scale\r\r\r\rSingle-cell RNA-Seq experiment\r\ra measurement of the gene expression profiles of individual cells.\r\r\r\rgene transcript\r\rAn RNA molecule of defined size over the length of a gene.\r\r\r\rcell lineage dynamics\r\rVisualized with tools such as scRNA-seq to track individual cells through their natural progression.\r\r\r\rFlow cytometry\r\rA technique for identifying and sorting cells and their components (such as DNA) by staining with a fluorescent dye and detecting the fluorescence usually by laser beam illumination\r\r\r\rMass cytometry\r\rA variation of flow cytometry in which antibodies are labeled with heavy metal ion tags rather than fluorochromes. Readout is by time-of-flight mass spectrometry.\r\r\r\rImmune cells\r\rcells that are part of the immune system and help the body fight infections and other diseases\r\r\r\rCD marker / antigen marker\r\rare specific types of molecules found on the surface of cells that help differentiate one cell type from another.\r\r\r\rCD4\r\rA glycoprotein found on the surface of immune cells such as T helper cells, monocytes, macrophages, and dendritic cells.\r\r\r\rhelper T cells\r\rA type of T cell that provides help to other cells in the immune response by recognizing foreign antigens and secreting substances called cytokines that activate T and B cells\r\r\r\rIsotope\r\rTwo or more forms of the same element that contain equal numbers of protons but different numbers of neutrons in their nuclei, and hence differ in relative atomic mass but not in chemical properties;\r\r\r\rInner cell mass (ICM)\r\rPluripotent cell lineage in the blastocyst. forms within the blastocyst, prior to its implantation within the uterus.\r\r\r\rBlastocyst\r\rA thin-walled hollow structure in early embryonic development that contains a cluster of cells called the inner cell mass from which the embryo arises.\r\r\r\rPluripotent epiblast (EPI)\r\rThe functional progenitors of soma and germ cells which later differentiate into three layers: definitive endoderm, mesoderm and ectoderm\r\r\r\rprimitive endoderm (PE)\r\rThe second extraembryonic tissue to form during embryogenesis in mammals. The PE develops from pluripotent cells of the blastocyst inner cell mass\r\r\r\rvariable regions\r\rin the context of taxon identification of bacteria bacterial 16S ribosomal RNA (rRNA) genes contain nine “hypervariable regions” (V1 – V9) that demonstrate considerable sequence diversity among different bacteria.\r\r\r\rChimera\r\rAn organism or tissue that contains at least two different sets of DNA, most often originating from the fusion of as many different zygotes (fertilized eggs).\r\r\r\r\rSources Consulted or Cited\rSome of the definitons above are based in part or whole on listed definitions in the following sources:\n\rHolmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press,\rCambridge, United Kingdom.\rWikipedia: The Free Encyclopedia. http://en.wikipedia.org/wiki/Main_Page\rhttps://academic.oup.com/biolreprod/article/85/5/946/2530522\rhttps://discovery.lifemapsc.com/in-vivo-development/inner-cell-mass/inner-cell-mass\rhttps://study.com/academy/lesson/inner-cell-mass-icm-definition-function-quiz.html\rhttps://www.sciencedirect.com/\rhttps://www.medicinenet.com/\rhttps://www.niaid.nih.gov/\rhttps://iti.stanford.edu/\rhttps://sysbiowiki.soe.ucsc.edu/node/323\rhttps://www.statisticshowto.datasciencecentral.com/between-group-variation/\rhttps://vsoch.github.io/2013/the-gap-statistic/\r\r\rPractice\r\r\r","date":1582243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582321282,"objectID":"e98258856605bd61f466b24e8c688fd1","permalink":"/post/vocabulary-for-chapter-5/","publishdate":"2020-02-21T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-5/","section":"post","summary":"Chapter 5 covers Clustering Analysis for large scale data anlysis like DNA/RNA sequencing outputs. These methods produce so much data that more unbiased approaches are required when attempting to make correlations.\n\r\r\r\r\r\r\r\runsupervised method\r\rA learning method where all variables are treated with the same status, rather than one variable being considered as an outcome or target.\r\r\r\rstatus\r\rA variable’s classification as an outcome/predictor (e.","tags":["Chapter 5","vocabulary"],"title":"Vocabulary for Chapter 5","type":"post"},{"authors":["Amy Fox"],"categories":["vocabulary","Chapter 4"],"content":"\r\rChapter 4 covers how to generate both finite and infinite mixture models from various distributions. It introduces a number of terms relating to these models. The vocabulary words for Chapter 4 are:\n\r\r\r\r\r\r\r\rfinite mixture\r\rin the context of statistic, when the distribution of interest is a combination of a few different probability distributions\r\r\r\rinfinite mixture\r\rin the context of statistic, when the distribution of interest is a combination of many probability distributions (as many or more probability distributions as observations)\r\r\r\rmixture model\r\ra model for a combination of two or more different probability distributions\r\r\r\rprobability density function\r\ra function giving the relative likelihood that a continuous random variable is equal to a given value. When this function is integrated over the sample space, it equals 1.\r\r\r\rbimodal distribution\r\ra distribution comprised of two modes\r\r\r\rexpectation-maximization (EM) algorithm\r\ran algorithm that allows for parameter estimation in probabilistic models with incomplete data\r\r\r\rdata augmentation\r\radding variables that are not measured (latent variables) to the data\r\r\r\rlatent variables\r\rvariables not measured in the data\r\r\r\rbivariate distribution\r\ra combined distribution made of two random variables\r\r\r\rmixture fraction\r\ra fraction used to describe the inhomogeneity in the mixture composition\r\r\r\ridentifiability\r\ran issue where there can be several explanations for the same observed values; occurs when there are too many degrees of freedom in parameters\r\r\r\rmarginal likelihood\r\rthe sum of the marginal distributions\r\r\r\rexpectation function\r\ra function that calculates the average of all possible values of the group that an observation belongs to\r\r\r\rmaximization step\r\ra step to optimize the parameters of a model\r\r\r\rsoft averaging\r\rthe process in which observations are not assigned to groups, rather they are added to multiple groups by using probabilities of memberships as weights\r\r\r\rmodel averaging\r\rthe process of using several models and combining them together into a weighted model\r\r\r\rzero-inflated data\r\rdata that contains a large number of zero counts\r\r\r\rChIP-Seq data\r\rsequencing data that identifies DNA binding sites for proteins\r\r\r\rchromosome\r\ra DNA molecule that contains the genetic material of an organism\r\r\r\rbinding site\r\rin the context of molecular biology, a specific region to which a macromolecule binds\r\r\r\rdeoxyribonucleotide monophosphate\r\ra single phosphate group in a unit of DNA\r\r\r\rgene expression measurement\r\rthe measurement of a functional gene product (i.e., protein or RNA)\r\r\r\rmicroarray\r\ra laboratory tool used to detect gene expression\r\r\r\rpromoter\r\rin the context of genetics, a region of DNA that initiates transcription of a gene\r\r\r\rpoint mass\r\ra finite probabiliity concentrated at a point in the proability mass distribution at which there is a discontinuous segment in probability density function\r\r\r\rsampling distribution\r\rthe probability distribution calculated from a random sample\r\r\r\rempirical cumulative distribution function (ECDF)\r\ra step distribution function based on empirical data measurements\r\r\r\rdensity\r\rin the context of probability distributions, the derivitive of the distribution function\r\r\r\rbootstrap\r\ran approximation of the true sampling distribution; created by drawing new samples from the empirical distribution of the original sample\r\r\r\rnon-parametric method\r\ra statistical method that does not make assumptions about population distribution or sample size\r\r\r\rnonparametric bootstrap\r\ran approximation of the true sampling distribution not based off of a specific assumption or a particular model\r\r\r\rLaplace distribution\r\ra distribution that shows differences between two independent variates with identical exponential distributions\r\r\r\rgamma distribution\r\ra distribution that is positively valued and continuous with two parameters: shape and scale\r\r\r\rnegative binomial distribution/ gamma-Poisson distubtion\r\rthe probability distribution of the number of failures before the kth success in a sequence of Bernoulli trials\r\r\r\rdispersion\r\rthe amount by which a set of observations deviate from their mean\r\r\r\rvariance-stabilizing transformations\r\rtransformations designed to give approximate independence between mean and variance\r\r\r\rheteroscedasticity\r\rthe variance of the data is different in different regions of the data\r\r\r\rdelta method\r\ra calculus procedure that uses random variables to approximate the expected value and variance of a function\r\r\r\r\rSources consulted or cited\rSome of the definitions above are based in part or whole on listed definitions\rin the following sources.\n\rHolmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press,\rCambridge, United Kingdom.\rEveritt and Skrondal, 2010. The Cambridge Dictionary of Statistics (Fourth Edition). Cambridge University Press, Cambridge, United Kingdom.\rZero-Inflated Poisson Regression. Institute for Digital Research and Education Statistical Consulting. https://stats.idre.ucla.edu/r/dae/zip/.\rBerrar, 2019. Introduction to Non-parametric Bootstrap. Research Gate. https://www.researchgate.net/\rDo and Batzoglou, 2008. What is the expectaion maximization algorithm?. Nature Biotechnology.\rWikipedia: The Free Encylcopedia. https://en.wikipedia.org/wiki/Main_Page\rGoogle Oxford American Dictionary. https://www.google.com\rd’Auzay, et al., 2019. Statistics of progress variable and mixture fraction gradients in an open turbulent jet spray flame. Fuel.\rBrownlee, 2019. A Gentle Introduction to Expectation-Maximization (EM Algorithm). Machine Learning Mastery. https://www.machinelearningmastery.com\rNon-parametric Methods. R tutorial. https://www.r-tutor.com\rPrecise analysis of DNA–protein binding sequences. Illumina. https://www.illumina.com\rMicroarray. Nature. https://www.nature.com\r\r\rPractice\r\r\r","date":1582156800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582156800,"objectID":"379ab172062419319ec4efa5c87e334a","permalink":"/post/vocabulary-for-chapter-4/","publishdate":"2020-02-20T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-4/","section":"post","summary":"Chapter 4 covers how to generate both finite and infinite mixture models from various distributions. It introduces a number of terms relating to these models. The vocabulary words for Chapter 4 are:\n\r\r\r\r\r\r\r\rfinite mixture\r\rin the context of statistic, when the distribution of interest is a combination of a few different probability distributions\r\r\r\rinfinite mixture\r\rin the context of statistic, when the distribution of interest is a combination of many probability distributions (as many or more probability distributions as observations)\r\r\r\rmixture model\r\ra model for a combination of two or more different probability distributions\r\r\r\rprobability density function\r\ra function giving the relative likelihood that a continuous random variable is equal to a given value.","tags":["vocabulary","Chapter 4"],"title":"Vocabulary for Chapter 4","type":"post"},{"authors":[],"categories":["quiz","Chapter 2"],"content":"\rThe vocabulary quiz will be live here during the start of the course.\nLoading…\r\r","date":1582070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582161527,"objectID":"ef44a84c1820dfdedb0a09c0a4ae4fb1","permalink":"/post/chapter-2-part-2-vocabulary-quiz/","publishdate":"2020-02-19T00:00:00Z","relpermalink":"/post/chapter-2-part-2-vocabulary-quiz/","section":"post","summary":"\rThe vocabulary quiz will be live here during the start of the course.\nLoading…\r\r","tags":["quiz","Chapter 2"],"title":"Chapter 2, part 2, vocabulary quiz","type":"post"},{"authors":["Sere Williams"],"categories":["exercises","Chapter 2"],"content":"\rAs always, load libraries first.\nlibrary(ggplot2)\rlibrary(tidyverse)\rlibrary(dplyr)\rExercise 2.3 from Modern Statistics for Modern Biologists\rA sequence of three nucleotides codes for one amino acid. There are 4 nucleotides, thus \\(4^3\\) would allow for 64 different amino acids, however there are only 20 amino acids requiring only 20 combinations + 1 for an “end” signal. (The “start” signal is the codon, ATG, which also codes for the amino acid methionine, so the start signal does not have a separate codon.) The code is redundant. But is the redundancy even among codons that code for the same amino acid? In other words, if alanine is coded by 4 different codons, do these codons code for alanine equally (each 25%), or do some codons appear more often than others? Here we use the tuberculosis genome to explore codon bias.\n\ra) Explore the data, mtb\rUse table to tabulate the AmAcid and Codon variables.\nEach amino acid is encoded by 1–6 tri-nucleotide combinations.\nmtb = read.table(\u0026quot;example_datasets/M_tuberculosis.txt\u0026quot;, header = TRUE)\rcodon_no \u0026lt;- rowSums(table(mtb))\rcodon_no\r## Ala Arg Asn Asp Cys End Gln Glu Gly His Ile Leu Lys Met Phe Pro Ser Thr Trp Tyr ## 4 6 2 2 2 3 2 2 4 2 3 6 2 1 2 4 6 4 1 2 ## Val ## 4\rThe PerThousands of each codon can be visualized, where each plot represents an amino acid and each bar represents a different codon that codes for that amino acid. But what does the PerThousands variable mean?\nggplot(mtb, aes(x=Codon, y=PerThous)) +\rgeom_col()+\rfacet_wrap(~AmAcid, scales=\u0026quot;free\u0026quot;) +\rtheme(axis.text.x = element_text(angle = 45, hjust = 1))\r\rb) The PerThous variable\rHow was the PerThous variable created?\nThe sum of all of the numbers of codons gives you the total number of codons in the M. tuberculosis genome: all_codons. Remember that this is not the size of the M. tuberculosis genome, but the number of codons in all M. tuberculosis genes. To get the size of the genome, multiply each codon by 3 (for each nucleotide) and add all non-coding nucleotides (which we do not know from this data set).\nall_codons = sum(mtb$Number)\rall_codons\r## [1] 1344223\rThe PerThousands variable is derived by dividing the number of occurrences of the codon of interest by the total number of codons. Because this number is small and hard to interpret, multiplying it by 1000 gives a value that is easy to make sense of. Here is an example for proline. The four values returned align to the four codons that each code for proline.\npro = mtb[mtb$AmAcid == \u0026quot;Pro\u0026quot;, \u0026quot;Number\u0026quot;]\rpro / all_codons * 1000\r## [1] 31.560240 6.121752 3.405685 17.032144\r\rc) Codon bias\rWrite an R function that you can apply to the table to find which of the amino acids shows the strongest codon bias, i.e., the strongest departure from uniform distribution among its possible spellings.\nFirst, let’s look at the expected frequencies of each codon.\ncodon_expected \u0026lt;- data.frame(codon_no) %\u0026gt;%\rrownames_to_column(var = \u0026quot;AmAcid\u0026quot;) %\u0026gt;%\rmutate(prob_codon = 1/codon_no)\rcodon_expected\r## AmAcid codon_no prob_codon\r## 1 Ala 4 0.2500000\r## 2 Arg 6 0.1666667\r## 3 Asn 2 0.5000000\r## 4 Asp 2 0.5000000\r## 5 Cys 2 0.5000000\r## 6 End 3 0.3333333\r## 7 Gln 2 0.5000000\r## 8 Glu 2 0.5000000\r## 9 Gly 4 0.2500000\r## 10 His 2 0.5000000\r## 11 Ile 3 0.3333333\r## 12 Leu 6 0.1666667\r## 13 Lys 2 0.5000000\r## 14 Met 1 1.0000000\r## 15 Phe 2 0.5000000\r## 16 Pro 4 0.2500000\r## 17 Ser 6 0.1666667\r## 18 Thr 4 0.2500000\r## 19 Trp 1 1.0000000\r## 20 Tyr 2 0.5000000\r## 21 Val 4 0.2500000\rNext, calculate the observed frequencies for each codon seen in the data set and use the chi-squared test statistic to determine if the difference between expected and observed codon frequencies is even or if some codon sequences are used more than others.\nTo start, you can group the data by amino acid and then determine a few things about\rthe amino acid or the possible codons for it, including the total observations\racross all codons for the amino acid (total), the number of codons for that\ramino acid (n_codons), and the expected count for each codon for that amino acid\r(the total number of observations for that amino acid divided by the number of\rcodons, giving an expected number that’s the same for all codons of an amino\racid; expected).\ncodon_compared \u0026lt;- mtb %\u0026gt;% group_by(AmAcid) %\u0026gt;% mutate(total = sum(Number),\rn_codons = n(),\rexpected = total / n_codons)\rcodon_compared\r## # A tibble: 64 x 7\r## # Groups: AmAcid [21]\r## AmAcid Codon Number PerThous total n_codons expected\r## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Gly GGG 25874 19.2 132810 4 33202.\r## 2 Gly GGA 13306 9.9 132810 4 33202.\r## 3 Gly GGT 25320 18.8 132810 4 33202.\r## 4 Gly GGC 68310 50.8 132810 4 33202.\r## 5 Glu GAG 41103 30.6 62870 2 31435 ## 6 Glu GAA 21767 16.2 62870 2 31435 ## 7 Asp GAT 21165 15.8 77852 2 38926 ## 8 Asp GAC 56687 42.2 77852 2 38926 ## 9 Val GTG 53942 40.1 114991 4 28748.\r## 10 Val GTA 6372 4.74 114991 4 28748.\r## # … with 54 more rows\rThe mutate function is used after group_by to do all this\rwithin each amino acid group of codons, but without collapsing to one row per\ramino acid, as a summarize call would.\nTo convince yourself that this has worked out correctly, you can repeat\rthe plot we made before and see that the bars for the expected values are\ralways equal across all codons for an amino acid:\nggplot(codon_compared, aes(x=Codon, y=expected)) +\rgeom_col()+\rfacet_wrap(~AmAcid, scales=\u0026quot;free\u0026quot;) +\rtheme(axis.text.x = element_text(angle = 45, hjust = 1))\rFinally, we can calculate the chi-squared (\\(\\chi^2\\)) statistic and compare it to the\rchi-squared distribution to get the p-value when testing against the null hypothesis\rthat the amino acid observations are uniformly distributed across codons. The \\(\\chi^2\\)\ris calculated as:\n\\[\r\\chi^2 = \\sum_i{\\frac{(O_i-E_i)^2}{E_i}}\r\\]\nwhere:\n\r\\(O_i\\) is the observed value of data point \\(i\\) (Number in our data); and\r\\(E_i\\) is the expected value of data point \\(i\\) (expected in our data)\r\rIn our data, we can calculate the contribution to the total \\(\\chi^2\\) statistic\rfrom each data point (in this case, each codon within an amino acid) using\rmutate, and then\radd these values up using group_by to group by amino acid followed by\rsummarize to sum up across all the data points for an amino acid.\rThe other information we need to get is the number of codons for the\ramino acid, because we’ll need this to determine the degrees of freedom\rfor the chi-squared distribution. Next, we used mutate with\rpchisq to determine the p-values within each amino acid group for the\rtest against the null that the codons are uniformly distributed for that\ramino acid (i.e., that there isn’t codon bias). These p-values turn out to\rbe super small, so we’re using a technique to get the log-transform versions of\rthem instead, which we explain a bit more later. Finally, we used arrange to\rlist the amino acids by evidence against uniform distribution of the codons,\rfrom most evidence against (smallest p-value so most negative log(p-value))\rto least evidence against (although still plenty of evidence against) and added\ran index with the ranking for each codon by adding a column with the sequence\rof numbers from 1 to the number of rows in the data (n()).\ncodon_compared %\u0026gt;% filter(n_codons \u0026gt; 1) %\u0026gt;% group_by(AmAcid) %\u0026gt;% mutate(chi_squared = ((Number - expected)^2/expected)) %\u0026gt;% summarise(chi_squared = sum(chi_squared),\rn = n()) %\u0026gt;% mutate(p_value = pchisq(chi_squared, df = n-1, log = TRUE, lower.tail = FALSE)) %\u0026gt;% arrange(p_value) %\u0026gt;% mutate(rank = 1:n())\r## # A tibble: 19 x 5\r## AmAcid chi_squared n p_value rank\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt;\r## 1 Leu 135432. 6 -67700. 1\r## 2 Ala 75620. 4 -37805. 2\r## 3 Arg 72183. 6 -36076. 3\r## 4 Thr 58767. 4 -29378. 4\r## 5 Val 58737. 4 -29363. 5\r## 6 Ile 56070. 3 -28035. 6\r## 7 Gly 52534. 4 -26262. 7\r## 8 Pro 45400. 4 -22695. 8\r## 9 Ser 36742. 6 -18357. 9\r## 10 Asp 16208. 2 -8109. 10\r## 11 Phe 13444. 2 -6727. 11\r## 12 Asn 11404. 2 -5707. 12\r## 13 Gln 9376. 2 -4693. 13\r## 14 Lys 6382. 2 -3195. 14\r## 15 Glu 5947. 2 -2978. 15\r## 16 His 5346. 2 -2678. 16\r## 17 Tyr 4738. 2 -2373. 17\r## 18 Cys 2958. 2 -1483. 18\r## 19 End 928. 3 -464. 19\rAs you may notice, these log transforms of the p-values (which we got rather than untransformed p-values in the pchisq call because we used the option log = TRUE) are large in magnitude and negative (so very tiny once you take the exponent if you re-transformed them to p-values) values. If you tried to calculate the untransformed p-values (and we did!), this number is so small (0.00000000e+00) that it is too small for R—it shows up as exactly zero in R, even though it actually is a very tiny, but still non-zero, number. To get around this issue, we told pchisq to work on these p-values as log transforms, and then we left the p-value as that log-transformed value. A group of numbers that are log transformed will be in the same order as their untransformed versions, so we don’t need to convert back to figure out which amino acid had that smallest p-value. We can just sort the amino acids from most negative to less negative using these log-transformed versions of the p-values. We now have the amino acids ranked from most biased codons (1) to least (19).\n\r","date":1582070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582151148,"objectID":"40f5d68248e48973d3f72a984ce57de5","permalink":"/post/exercise-solution-for-chapter-2/","publishdate":"2020-02-19T00:00:00Z","relpermalink":"/post/exercise-solution-for-chapter-2/","section":"post","summary":"As always, load libraries first.\nlibrary(ggplot2)\rlibrary(tidyverse)\rlibrary(dplyr)\rExercise 2.3 from Modern Statistics for Modern Biologists\rA sequence of three nucleotides codes for one amino acid. There are 4 nucleotides, thus \\(4^3\\) would allow for 64 different amino acids, however there are only 20 amino acids requiring only 20 combinations + 1 for an “end” signal. (The “start” signal is the codon, ATG, which also codes for the amino acid methionine, so the start signal does not have a separate codon.","tags":["exercises","Chapter 2"],"title":"Exercise solution for Chapter 2, Part 1","type":"post"},{"authors":["Brooke Anderson"],"categories":["quiz","Chapter 2"],"content":"\rThe vocabulary quiz will be live here during the start of the course.\nLoading…\r\r","date":1581552000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581612248,"objectID":"7c8103a2670fda3b5cb4a7557b69a29c","permalink":"/post/chapter-2-vocabulary-quiz/","publishdate":"2020-02-13T00:00:00Z","relpermalink":"/post/chapter-2-vocabulary-quiz/","section":"post","summary":"\rThe vocabulary quiz will be live here during the start of the course.\nLoading…\r\r","tags":["quiz","Chapter 2"],"title":"Chapter 2 Part 1 vocabulary quiz","type":"post"},{"authors":["Brooke Anderson"],"categories":["instructions","blogdown","exercises","github"],"content":"\rEach of you will be responsible once or twice over the semester to create\ra blog post that provides a clean, clearly-presented solution to the\rin-class exercise for the week. This blog post provides the technical\rinstructions for writing and submitting that exercise.\nYour exercise solution should be posted before the next class meeting.\rSince it will need to be reviewed by the faculty before it can be officially\rposted, please plan to submit it by the Tuesday after the class for your\rexercise. Student assignments for the exercises are given in the\rSchedule section of our course website.\nOverview of creating a post\rYou will be submitting your exercise solution as a blog post. Creating\rone for our website will follow all the same steps as creating a blog\rpost for a vocabulary list, just with different content. Please read\rthe post on creating a vocabulary list\rand follow the steps there to:\n\rUpdate your fork of the website\rMake a new blog post\rUse RMarkdown syntax to write the blog post\rSubmit the blog post\r\r\rContent for the blog post\rThe blog post should provide a walk-through of the solution to that week’s in-course\rexercise. We have posted an example for the exercise for Chapter 1\rto give you an idea of what you should aim to write.\nGenerally, this exercise will be a resource for everyone in the class, to make sure\rthey’ve understood the exercise, as well as to see how someone else tackled the problem.\rYour solution should cover all parts of the exercise (for example, if there’s a\rpart A and B, you should cover both). You can start by writing it as you would if you\rwere assigned the exercise as a homework problem, but then you should do a second step\rof revision to provide some context and dig a bit deeper into how you tackled\rthe question. Since we are only requiring you to write up exercise answers once\ror twice over the semester (rather than submitting homework for exercises every\rweek), we expect this product to be more in-depth and polished than a typical\rhomework solution.\nFirst, make sure that you have provided text explaining what the\rexercise asks for, in case the reader hasn’t recently read the exercise prompt.\rSecond, please add a few details either about how you tackled the problem through code\ror how the statistical principles covered in the exercise could apply to other problems\ryou’ve come across in your research or coursework.\nTo help in preparing your post, plan to spend the exercise time in class during the\rweek of your exercise visiting the different groups of students working on the\rexercise. You can talk to them about how they’re approaching the problem, how they\rinterpret it, etc., to help you develop your own answer.\n\rTips\r\rBe sure to refresh yourself on all the Markdown formatting tags you can use to improve\rthe appearance of your post. Be sure to include things like section headings and\ritalics or bold as appropriate. RStudio’s website has some nice cheatsheets on\rRMarkdown that can help.\rMake sure you include R code if appropriate. If you put parentheses around an\rassignment expression in R, it will print out the assigned object and make the\rassignment in the same call—you might find this useful in writing concise code\rwhile still showing what’s in the objects you create.\rUse the $ and $$ tags in RMarkdown to include mathematical equations in your blog post\rwhen appropriate.\rIf you need to read in a dataset for R code in your blog post, save it in the\rwebsite directory’s “content/post/example_datasets” subdirectory. If your data\rcomes from an online source or from an R library, you won’t need to do this,\ronly if you need a “local” copy of the datafile to run your RMarkdown code.\rYou are welcome to draw from (and cite) other statistics textbooks or dictionaries\rif you’d like to in explaining the problem and your approach to it.\rFor the code, look at vignettes and helpfiles, especially for packages you are not\rfamiliar with.\rFor a lot of Bioconductor packages, object-oriented programming is used pretty\rheavily. This means that associated data in R packages will often be stored in a\rformat that you haven’t used yet. Look up more information on data classes used in\ryour exercise if you aren’t familiar with them. You can use the class function\rto determine the class of an object as well as the name of the package that defines\rthat class. The str function is often helpful for exploring a data object class, as well.\rMany of the Bioconductor object classes will have special accessor methods, which are\rfunctions that allow you to extract certain elements from the object—check the helpfile\rfor the object class, as these methods are often listed there with examples.\rGoogling can also be very helpful for learning more about functions, packages, and\rdatasets in R, especially if you don’t yet know what package the item is from.\rMost Bioconductor packages have very nice vignettes available online and from your\rR session once you have installed the package. These are a great place to start to find\rout more about how to use the functions and object classes that come with the package.\r\r\r","date":1581552000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581477683,"objectID":"459571b5f2972f118129cd8d9a5201a1","permalink":"/post/how-to-create-an-exercise-solution-blog-post/","publishdate":"2020-02-13T00:00:00Z","relpermalink":"/post/how-to-create-an-exercise-solution-blog-post/","section":"post","summary":"Each of you will be responsible once or twice over the semester to create\ra blog post that provides a clean, clearly-presented solution to the\rin-class exercise for the week. This blog post provides the technical\rinstructions for writing and submitting that exercise.\nYour exercise solution should be posted before the next class meeting.\rSince it will need to be reviewed by the faculty before it can be officially\rposted, please plan to submit it by the Tuesday after the class for your\rexercise.","tags":["instructions","blogdown","exercises","github"],"title":"How to create an exercise solution blog post","type":"post"},{"authors":["Sierra Pugh"],"categories":["Chapter 2","vocabulary"],"content":"\r\rThese sections introduced Markov chains and the Bayesian paradigm. Markov chain transitions were used to model dependencies along DNA sequences. The vocabulary terms are:\n\r\r\r\r\r\r\r\rMarkov chain\r\ra sequence where given the current state, the next state is conditionally independent of all previous states\r\r\r\rBayesian paradigm\r\rapproaching statistics from the perspective that probability can be viewed as a degree of belief in an event\r\r\r\rBeta distribution\r\ra probability distribution defined on the interval [0, 1] often used to model probabilities in Bayesian statistics\r\r\r\rExponential distribution\r\ra probability distribution defined on the positive real numbers that can be used to model the time between events in a Poisson point process\r\r\r\rPrior\r\ra probability distribution describing our knowledge of a hypothesis/parameter before incorporating new data\r\r\r\rPosterior\r\ra probability distribution describing our knowledge of a hypothesis/parameter after incorporating new data\r\r\r\rHaplotype\r\ra collection of DNA sequence variants (e.g., alleles) that are spatially close on a chromosome, are usually inherited together, and thus are genetically linked\r\r\r\rMarginal distribution\r\rthe distribution of a sub-collection of variables after integrating out the remaining variables in the collection.\r\r\r\rMonte Carlo integration\r\ra technique for numerical integration where the value of an integral is estimated by simulating data\r\r\r\rQuantile-quantile plot (QQ-plot)\r\ra plot comparing the quantiles from one distribution (often a theoretical distribution) to the quantiles of another distribution (often from a sample)\r\r\r\rMaximum a posteriori (MAP) estimate\r\rthe mode of the posterior distribution associated with the quantity of interest\r\r\r\rEscherichia coli\r\rfacultative anaerobic, rod-shaped, coliform bacterium commonly found in the lower intestine of warm-blooded organisms\r\r\r\rEpigenetics\r\rthe study of heritable phenotype changes that do not involve alterations in the DNA sequence\r\r\r\rLog-likelihood ratio\r\rthe log of the likelihood under one set of assumptions divided by the likelihood under another set of assumptions\r\r\r\rBimodality\r\rwhen a distribution has two modes\r\r\r\rMixture\r\rin the context of statistics, when the distribution of interest is a combination of two or more different probability distributions\r\r\r\rCodon\r\rA three-nucleotide sequence that specifies the amino acid to be created next (or to start or stop synthesis)\r\r\r\rCodon bias\r\rthe differences in how often each spelling of an amino acid occurs in coding DNA\r\r\r\rGenetic code\r\rthe set of instructions in a gene that tell the cell how to make a specific protein\r\r\r\r\rSources consulted or cited\rSome of the definitons above are based in part or whole on listed definitions in the following sources:\n\rHolmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press,\rCambridge, United Kingdom.\rWikipedia: The Free Encyclopedia. http://en.wikipedia.org/wiki/Main_Page\rNIH Genetics Home Reference. https://ghr.nlm.nih.gov/\rNCBI Genetics Review. https://www.ncbi.nlm.nih.gov\r\r\rPractice\r\r\r","date":1581552000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581691815,"objectID":"6cf732c0340c1c0a7b23cd9a5e5104b6","permalink":"/post/vocabulary-for-chapter-2-8-2-12/","publishdate":"2020-02-13T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-2-8-2-12/","section":"post","summary":"These sections introduced Markov chains and the Bayesian paradigm. Markov chain transitions were used to model dependencies along DNA sequences. The vocabulary terms are:\n\r\r\r\r\r\r\r\rMarkov chain\r\ra sequence where given the current state, the next state is conditionally independent of all previous states\r\r\r\rBayesian paradigm\r\rapproaching statistics from the perspective that probability can be viewed as a degree of belief in an event\r\r\r\rBeta distribution\r\ra probability distribution defined on the interval [0, 1] often used to model probabilities in Bayesian statistics\r\r\r\rExponential distribution\r\ra probability distribution defined on the positive real numbers that can be used to model the time between events in a Poisson point process\r\r\r\rPrior\r\ra probability distribution describing our knowledge of a hypothesis/parameter before incorporating new data\r\r\r\rPosterior\r\ra probability distribution describing our knowledge of a hypothesis/parameter after incorporating new data\r\r\r\rHaplotype\r\ra collection of DNA sequence variants (e.","tags":["Chapter 2","vocabulary"],"title":"Vocabulary for Chapter 2, Part 2","type":"post"},{"authors":["Brooke Anderson"],"categories":["exercises","Chapter 1"],"content":"\rThis exercise asks us to explore the frequency of each of the four nucleotides\r(A, C, G, and T) in the genome of C. elegans, a type of worm used frequently\rin scientific research.\nThis solution requires that several R extension packages be loaded in your R\rsession. If you do not have these packages installed to your computer yet, you\rshould follow instructions we’ve posted\rseparately\rdescribing the required set-up for this exercise. Once you have installed these\rpackages on your computer, you can load them into your current R session using\rthe library function:\nlibrary(\u0026quot;BSgenome.Celegans.UCSC.ce2\u0026quot;)\rlibrary(\u0026quot;Biostrings\u0026quot;)\rlibrary(\u0026quot;tidyverse\u0026quot;)\rlibrary(\u0026quot;knitr\u0026quot;)\rPart A\rPart A of the question asks us to explore the nucleotide frequency of the C.\relegans genome. This genome is available in the Celegans data that comes with\rthe BSgenome.Clegans.UCSC.ce2 package and is stored within a BSgenome class,\rwhich is a special object class provided by the Biostrings package.\nThere is a dedicated function called letterFrequency in the Biostrings\rpackage that can be used to count the frequency of letters in a string (like a\rgenome) in an R object like this. In a call to this function, you must also\rinclude the possible letters in your “alphabet”—that is, the possible letters\rthat each position in your string could take.\n(nuc_freq \u0026lt;- letterFrequency(Celegans$chrM, letters=c(\u0026quot;A\u0026quot;, \u0026quot;C\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;T\u0026quot;)))\r## A C G T ## 4335 1225 2055 6179\rTo explore and plot this data, I put this summary data into a tibble, so I\rcould more easily use tidyverse tools with the data.\nnuc_freq_df \u0026lt;- tibble(nucleotide = names(nuc_freq), n = nuc_freq)\rnuc_freq_df\r## # A tibble: 4 x 2\r## nucleotide n\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 A 4335\r## 2 C 1225\r## 3 G 2055\r## 4 T 6179\rIn this format, you can use tidyverse tools to explore the data a bit more.\rFor example, you can determine the total number of nucleotides in the genome\rand, with that calculate the proportion of each nucleotide across the genome.\rAlong with the kable function from the knitr package, I created a formatted\rtable with this information:\nnuc_freq_df %\u0026gt;% mutate(prop = n / sum(n)) %\u0026gt;% kable(digits = 2, caption = \u0026quot;Nucleotide frequencies and proportions in *C. elegans*\u0026quot;,\rcol.names = c(\u0026quot;Nucleotide\u0026quot;, \u0026quot;Frequency\u0026quot;, \u0026quot;Proportion\u0026quot;))\r\rTable 1: Nucleotide frequencies and proportions in C. elegans\r\rNucleotide\rFrequency\rProportion\r\r\r\rA\r4335\r0.31\r\rC\r1225\r0.09\r\rG\r2055\r0.15\r\rT\r6179\r0.45\r\r\r\rFor some presentations, it might be clearer to present this information in a\rslightly different table format, using pivot_longer and then pivot_wider to\rreformat the table for presentation:\nnuc_freq_df %\u0026gt;% mutate(prop = n / sum(n),\rn = prettyNum(n, big.mark = \u0026quot;,\u0026quot;),\rprop = prettyNum(prop, digits = 2)) %\u0026gt;% pivot_longer(cols = c(\u0026quot;n\u0026quot;, \u0026quot;prop\u0026quot;)) %\u0026gt;% pivot_wider(names_from = \u0026quot;nucleotide\u0026quot;) %\u0026gt;% mutate(name = case_when(\rname == \u0026quot;n\u0026quot; ~ \u0026quot;Frequency of nucleotide\u0026quot;,\rname == \u0026quot;prop\u0026quot; ~ \u0026quot;Proportion of all nucleotides\u0026quot;\r)) %\u0026gt;% rename(` ` = name) %\u0026gt;% kable(align = c(\u0026quot;rcccc\u0026quot;), caption = \u0026quot;Nucleotide frequencies and proportions in *C. elegans*\u0026quot;)\r\rTable 2: Nucleotide frequencies and proportions in C. elegans\r\r\rA\rC\rG\rT\r\r\r\rFrequency of nucleotide\r4,335\r1,225\r2,055\r6,179\r\rProportion of all nucleotides\r0.31\r0.089\r0.15\r0.45\r\r\r\rHere is a plot of the frequency of each of the four nucleotides for the C.\relegans nucleotide:\nggplot(nuc_freq_df, aes(x = nucleotide, y = n)) + geom_col(fill = \u0026quot;lavender\u0026quot;, color = \u0026quot;black\u0026quot;) + theme_classic() + scale_y_continuous(label = scales::comma) + theme(axis.title = element_blank()) + labs(title = expression(paste(italic(\u0026quot;C. elegans\u0026quot;), \u0026quot; neucleotide frequency\u0026quot;)),\rcaption = expression(paste(\u0026quot;Based on data from the \u0026quot;, italic(\u0026quot;BSgenome.Celegans.UCSC.ce2\u0026quot;), \u0026quot; package.\u0026quot;)))\rThis graph uses a few elements to improve its appearance that you might want to\rexplore if you’re not already familiar with them:\n\rThe labs function is used to add both a title and a caption to the plot.\rThe paste, expression, and italic functions are used together to put “C.\relegans” and an R package name in italics in some of the labels on the plot.\rThe scales package is used inside a scale layer for the ggplot2 code to\rmake the y-axis labels a bit nicer.\rtheme calls are used to apply a simpler overall theme than the default and to\rremove the x- and y-axis titles (with element_blank).\rThe color and fill of the bars are customized in the geom layer (geom_col).\r\rFrom this plot, it certainly looks like the nucleotides are not uniformly\rdistributed in the C. elegans genome. This question will be investigated more\rin the next part of the exercise.\n\rPart B\rThe second part of the exercise asks us to test whether the observed nucleotide\rdata for C. elegans is consistent with the uniform model that all nucleotide\rfrequencies are the same.\nFirst, we can simulate several datasets under this null model and see how a plot\rof nucleotide frequencies compares to the plot that we obtained with the observed\rC. elegans data. To make these plots, I first simulated 20 samples under the\rnull model that the distribution is uniform across the four nucleotides, using\rthe rmultinom function with the size argument set to the number of nucleotides in\rthe original C. elegans genome data and the prob argument set to have an equal\rprobability of each nucleotide at each spot on the genome:\n(sim_nuc_freq \u0026lt;- rmultinom(n = 20, size = sum(nuc_freq_df$n), prob = rep(1 / 4, 4)))\r## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\r## [1,] 3494 3353 3406 3407 3383 3508 3385 3383 3428 3391 3506 3377 3400 3517\r## [2,] 3459 3506 3422 3501 3401 3396 3417 3472 3493 3489 3448 3436 3465 3506\r## [3,] 3428 3429 3542 3403 3462 3381 3508 3477 3386 3523 3402 3530 3461 3303\r## [4,] 3413 3506 3424 3483 3548 3509 3484 3462 3487 3391 3438 3451 3468 3468\r## [,15] [,16] [,17] [,18] [,19] [,20]\r## [1,] 3501 3435 3415 3440 3488 3417\r## [2,] 3468 3584 3500 3500 3447 3447\r## [3,] 3417 3401 3401 3366 3431 3457\r## [4,] 3408 3374 3478 3488 3428 3473\rNext, I moved this into a tibble so I could more easily rearrange and plot the data using\rfacetting in ggplot2:\nsim_nuc_freq_df \u0026lt;- as_tibble(sim_nuc_freq) %\u0026gt;% mutate(nucleotide = c(\u0026quot;A\u0026quot;, \u0026quot;C\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;T\u0026quot;)) %\u0026gt;% pivot_longer(-nucleotide, names_to = \u0026quot;sample\u0026quot;) %\u0026gt;% mutate(sample = sample %\u0026gt;% str_remove(\u0026quot;V\u0026quot;) %\u0026gt;% as.numeric()) %\u0026gt;% arrange(sample, nucleotide)\rsim_nuc_freq_df %\u0026gt;% slice(1:10)\r## # A tibble: 10 x 3\r## nucleotide sample value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt;\r## 1 A 1 3494\r## 2 C 1 3459\r## 3 G 1 3428\r## 4 T 1 3413\r## 5 A 2 3353\r## 6 C 2 3506\r## 7 G 2 3429\r## 8 T 2 3506\r## 9 A 3 3406\r## 10 C 3 3422\rggplot(sim_nuc_freq_df, aes(x = nucleotide, y = value)) + geom_col(fill = \u0026quot;lavender\u0026quot;, color = \u0026quot;black\u0026quot;) + theme_classic() + scale_y_continuous(label = scales::comma) + theme(axis.title = element_blank()) + labs(title = \u0026quot;Simulated neucleotide frequencies under a uniform model\u0026quot;) +\rfacet_wrap(~ sample) + expand_limits(y = max(nuc_freq_df$n))\rThe y-axis limits were expanded here to cover the same range as that shown for the\robserved C. elegans nucleotide frequencies, to help make it easier to compare these plots\rwith the plot of our observed data. These plots of data simulated under the null model do\rshow some variation in frequencies among the nucleotides, but it’s certainly much less than\rin the observed data for C. elegans.\nNext, I repeated this simulation process, but I increased the number of simulations to 1,000:\nsim_nuc_freq_df \u0026lt;- rmultinom(n = 1000, size = sum(nuc_freq_df$n), prob = rep(1 / 4, 4)) %\u0026gt;% as_tibble() %\u0026gt;% mutate(nucleotide = c(\u0026quot;A\u0026quot;, \u0026quot;C\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;T\u0026quot;)) %\u0026gt;% pivot_longer(-nucleotide, names_to = \u0026quot;sample\u0026quot;) %\u0026gt;% mutate(sample = sample %\u0026gt;% str_remove(\u0026quot;V\u0026quot;) %\u0026gt;% as.numeric()) %\u0026gt;% arrange(sample, nucleotide)\rsim_nuc_freq_df %\u0026gt;% slice(1:10)\r## # A tibble: 10 x 3\r## nucleotide sample value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt;\r## 1 A 1 3418\r## 2 C 1 3481\r## 3 G 1 3459\r## 4 T 1 3436\r## 5 A 2 3531\r## 6 C 2 3404\r## 7 G 2 3435\r## 8 T 2 3424\r## 9 A 3 3332\r## 10 C 3 3477\rUsing this dataframe of simulations, we can measure the mean, minimum, and maximum frequencies\rof each nucleotide across all 1,000 simulations:\n(sim_summary \u0026lt;- sim_nuc_freq_df %\u0026gt;% group_by(nucleotide) %\u0026gt;% summarize(mean_freq = mean(value),\rmin_freq = min(value), max_freq = max(value)))\r## # A tibble: 4 x 4\r## nucleotide mean_freq min_freq max_freq\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 A 3446. 3290 3595\r## 2 C 3450. 3287 3634\r## 3 G 3449. 3270 3644\r## 4 T 3448. 3252 3613\rTo help compare this with the observed data, we can create a table with information from\rboth the original data and the simulations under the null model:\nnuc_freq_df %\u0026gt;% left_join(sim_summary, by = \u0026quot;nucleotide\u0026quot;) %\u0026gt;% mutate_at(c(\u0026quot;mean_freq\u0026quot;, \u0026quot;min_freq\u0026quot;, \u0026quot;max_freq\u0026quot;, \u0026quot;n\u0026quot;), prettyNum, big.mark = \u0026quot;,\u0026quot;, digits = 0) %\u0026gt;% mutate(simulations = paste0(mean_freq, \u0026quot; (\u0026quot;, min_freq, \u0026quot;, \u0026quot;, max_freq, \u0026quot;)\u0026quot;)) %\u0026gt;% select(nucleotide, n, simulations) %\u0026gt;% kable(col.names = c(\u0026quot;Nucleotide\u0026quot;, \u0026quot;Frequency in C. elegans genome\u0026quot;,\r\u0026quot;Mean frequency (minimum frequency, maximum frequency) across 1,000 simulations\u0026quot;), align = \u0026quot;c\u0026quot;)\r\r\rNucleotide\rFrequency in C. elegans genome\rMean frequency (minimum frequency, maximum frequency) across 1,000 simulations\r\r\r\rA\r4,335\r3,446 (3,290, 3,595)\r\rC\r1,225\r3,450 (3,287, 3,634)\r\rG\r2,055\r3,449 (3,270, 3,644)\r\rT\r6,179\r3,448 (3,252, 3,613)\r\r\r\rThis helps clarify how unusual the observed data would be under the null model—the\rcounts of all four nucleotides in the C. elegans genome are completely outside the\rrange of frequencies in the simulated data.\nAnother way to look at this is with histograms of the distribution of frequencies\rof each nucleotide under the null model compared to the observed frequencies in\rthe C. elegans nucleotide:\nggplot(sim_nuc_freq_df, aes(x = value)) + geom_histogram(binwidth = 10) + facet_wrap(~ nucleotide) + theme_classic() + scale_x_continuous(name = \u0026quot;Frequency of nucleotide in the simulation under the null model\u0026quot;,\rlabels = scales::comma) + scale_y_continuous(name = \u0026quot;# of simulations (out of 1,000)\u0026quot;) + geom_vline(data = nuc_freq_df, aes(xintercept = n), color = \u0026quot;red\u0026quot;) + labs(title = expression(paste(\u0026quot;Nucleotide frequency in \u0026quot;,\ritalic(\u0026quot;C. elegans\u0026quot;), \u0026quot; compared null model simulations\u0026quot;)),\rcaption = \u0026quot;Red line shows the frequency observed for the nucleotide in C. elegans\u0026quot;)\rFinally, to help in answering this question, it would be interesting to look at a\rsingle measure for each simulation (and for the observed data) rather than comparing\reach nucleotide one at a time. Chapter 1 gives the equation for a statistic to\rmeasure variability in multinomial data by calculating the sum of squares for the\rdifferences between the observed and expected count of nucleotides for each of the\rfour nucleotides in a sample (p. 12).\nI calculated this statistic for the observed data and then for each of the 1,000\rsimulations.\n(obs_stat \u0026lt;- nuc_freq_df %\u0026gt;% mutate(expected = mean(n),\rstat_input = (n - expected) ^ 2 / expected) %\u0026gt;% summarize(variability_stat = sum(stat_input)))\r## # A tibble: 1 x 1\r## variability_stat\r## \u0026lt;dbl\u0026gt;\r## 1 4387.\rsim_stat \u0026lt;- sim_nuc_freq_df %\u0026gt;% mutate(expected = mean(value), stat_input = (value - expected) ^ 2 / expected) %\u0026gt;% group_by(sample) %\u0026gt;% summarize(variability_stat = sum(stat_input))\rsim_stat %\u0026gt;% slice(1:5)\r## # A tibble: 5 x 2\r## sample variability_stat\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 0.653\r## 2 2 2.77 ## 3 3 6.44 ## 4 4 3.93 ## 5 5 5.14\rHere is a plot of the distribution of this statistic across the 1,000 simulations:\nggplot(sim_stat, aes(x = variability_stat)) + geom_rect(data = sim_stat, aes(xmin = quantile(variability_stat, prob = 0.025),\rxmax = quantile(variability_stat, prob = 0.975),\rymin = 0, ymax = Inf), fill = \u0026quot;beige\u0026quot;, alpha = 0.5) +\rgeom_histogram(bins = 30, fill = \u0026quot;white\u0026quot;, color = \u0026quot;tan\u0026quot;, alpha = 0.5) +\rtheme_classic() + labs(title = \u0026quot;Variability from expected values\u0026quot;,\rsubtitle = \u0026quot;Values from simulations under the null\u0026quot;,\rx = \u0026quot;Value of variability statistic\u0026quot;, y = \u0026quot;Number of simulations with given value\u0026quot;,\rcaption = \u0026quot;The shaded yellow area shows the region of the central 95% of\\nstatistic values for the 1,000 simulations under the null model.\u0026quot;)\rThe value of this statistic for the observed nucleotide frequencies for C.\relegans is 4387, which is much larger (indicating greater variability\rfrom expected values under the null model) than the value observed under most of\rthe simulations. It is, in fact, far outside the central 95% range of values\robserved in simulations.\n\r","date":1581379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581477727,"objectID":"22f4b300c757465ac6d281137e95056c","permalink":"/post/exercise-solution-for-chapter-1/","publishdate":"2020-02-11T00:00:00Z","relpermalink":"/post/exercise-solution-for-chapter-1/","section":"post","summary":"This exercise asks us to explore the frequency of each of the four nucleotides\r(A, C, G, and T) in the genome of C. elegans, a type of worm used frequently\rin scientific research.\nThis solution requires that several R extension packages be loaded in your R\rsession. If you do not have these packages installed to your computer yet, you\rshould follow instructions we’ve posted\rseparately\rdescribing the required set-up for this exercise.","tags":["exercises","Chapter 1"],"title":"Exercise solution for Chapter 1","type":"post"},{"authors":["Bailey Fosdick"],"categories":["Chapter 2","vocabulary"],"content":"\r\rThe first portion of Chapter 2 (2.1-2.7) is focused on statistical modeling of data. It introduces a number of distributions commonly used in statistics, as well as model fitting estimation procedures (e.g. maximum likelihood estimation).\nThe vocabulary words for Chapter 2, part 1, are:\r\r\r\r\r\r\r\r\rstatistical inference / up / statistical approach\r\rAn upward-reasoning approach that start with data and works towards defining a model that might possibly explain the data.\r\r\r\rdeduction\r\rStarting from a mathematical/statistical model with known parameters and computing the probability of observing an event.\r\r\r\rnull model\r\rThe model associated with the null hypothesis, which formulates an “uninteresting” baseline.\r\r\r\rgoodness-of-fit\r\rEvaluation of whether a theorectical distribution/model is appropriate for a data set.\r\r\r\rrootogram\r\rDiagram to assess model goodness-of-fit for a data set. Bar chart where the bars “hang” from their theorectical values and will approximately line up with horizontal axis if the model is a good fit to the data.\r\r\r\rmaximum likelihood estimator (MLE)\r\rA rule, or mathematical formula, that outputs an estimate of a parameter for a model, where that estimate maximizes the probability of the observed data.\r\r\r\rconservative (approach)\r\rAn analysis approach that errs on the side of caution to avoid concluding an alternative hypothesis (e.g. detecting a signal) when it is not true.\r\r\r\rvectorization\r\rIn regard to function evaluation, if a vector is supplied to a function that expects a scalar, R will apply the function to each element of the vector.\r\r\r\rlikelihood function\r\rThe probability of the data under a model expressed as a function of the model parameter(s).\r\r\r\restimation\r\rProcess of using data to perform inference on population parameters.\r\r\r\rstatistical testing\r\rFormal decision process to determine if a null model is appropriate for the observed data.\r\r\r\rregression\r\rRelating how an outcome measure depends on one or more covariates.\r\r\r\rresidual\r\rDeviation between the observed data and the expected value of the data point according to a model.\r\r\r\rgeneralized linear model\r\rA class of models for non-continuous or non-negative data that allows regression of an outcome on observed covariates. An extension of linear regression.\r\r\r\rchi-squared distribution\r\rA distribution on the non-negative real numbers that is often used in assessing goodness-of-fit (e.g. models fit to contingency tables).\r\r\r\rquantile-quantile (QQ) plot\r\rUsed to compare two distributions (or samples). Deviations in the plot from the y=x line suggest differences between the two distributions.\r\r\r\rquantile\r\rValue corresponding to a percentile of a distribution.\r\r\r\rempirical cumulative distribution function (ECDF)\r\rFunction with input value x gives as output the probability that a random variable from the distribution is less than or equal to x. Function is defined using a sample and assigning probability 1/n to each data point.\r\r\r\rchi-squared statistic\r\rA summary statistic of a data set that has a theorectical chi-squared distribution.\r\r\r\rbase pairing\r\rThe pattern that adenine (A) and thymine (T) are paired (appear with equal frequency) in the DNA of an organism, and similarly cytosine (C) and guianine (G) are paired.\r\r\r\rcontingency table\r\rTable of counts summarizing the number of times combinations of factor levels were observed in the data set.\r\r\r\rHardy-Weinberg equilibrium (HWE)\r\rAssuming random mating, this principle characterizes the distribution of genotype frequencies as a function of the relative frequencies of each allele.\r\r\r\rposition weight matrix (PWM) / position-specific scoring matrix (PSSM)\r\rTable giving the probability of each nucleotide at each position\r\r\r\rsequence logo\r\rA graphical summary of the position weight matrix or position-specific scoring matrix.\r\r\r\r\rPractice\r\r\r","date":1581206400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581281287,"objectID":"2788b54b4b00409516daea264740162d","permalink":"/post/vocabularly-for-chapter-2-part-1/","publishdate":"2020-02-09T00:00:00Z","relpermalink":"/post/vocabularly-for-chapter-2-part-1/","section":"post","summary":"Vocabulary for the first part of Chapter 2","tags":["Chapter 2","vocabulary"],"title":"Vocabularly for Chapter 2, Part 1","type":"post"},{"authors":["Bailey Fosdick"],"categories":["Chapter 1","exercises"],"content":"\rThe code instructions in the exercise statement appear to be outdated. The code below worked on my machine. Note that when asked whether I would like to update packages from the binary version, I said no. (When I said yes, R gave an error.)\nif (!requireNamespace(\u0026quot;BiocManager\u0026quot;, quietly = TRUE))\rinstall.packages(\u0026quot;BiocManager\u0026quot;)\rBiocManager::install(c(\u0026quot;Biostrings\u0026quot;, \u0026quot;BSgenome.Celegans.UCSC.ce2\u0026quot;,\u0026quot;BSgenome\u0026quot;))\rYou can see the various data genome data sets available by loading the BSgenome library and typing available.genomes().\nOnce you have the needed packages installed, you can access the sequence data for this exercise via the following commands.\nsuppressMessages(library(\u0026quot;BSgenome.Celegans.UCSC.ce2\u0026quot;))\rCelegans\r## Worm genome:\r## # organism: Caenorhabditis elegans (Worm)\r## # provider: UCSC\r## # provider version: ce2\r## # release date: Mar. 2004\r## # release name: WormBase v. WS120\r## # 7 sequences:\r## # chrI chrII chrIII chrIV chrV chrX chrM ## # (use \u0026#39;seqnames()\u0026#39; to see all the sequence names, use the \u0026#39;$\u0026#39; or \u0026#39;[[\u0026#39; operator\r## # to access a given sequence)\rseqnames(Celegans)\r## [1] \u0026quot;chrI\u0026quot; \u0026quot;chrII\u0026quot; \u0026quot;chrIII\u0026quot; \u0026quot;chrIV\u0026quot; \u0026quot;chrV\u0026quot; \u0026quot;chrX\u0026quot; \u0026quot;chrM\u0026quot;\rCelegans$chrM\r## 13794-letter \u0026quot;DNAString\u0026quot; instance\r## seq: CAGTAAATAGTTTAATAAAAATATAGCATTTGGGTT...TATTTATAGATATATACTTTGTATATATCTATATTA\rclass(Celegans$chrM)\r## [1] \u0026quot;DNAString\u0026quot;\r## attr(,\u0026quot;package\u0026quot;)\r## [1] \u0026quot;Biostrings\u0026quot;\rlength(Celegans$chrM)\r## [1] 13794\rThe Biostrings packages provides functions to summarize the sequence. For example:\nlibrary(\u0026quot;Biostrings\u0026quot;)\rlfM = letterFrequency(Celegans$chrM, letters=c(\u0026quot;A\u0026quot;, \u0026quot;C\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;T\u0026quot;))\rlfM\r## A C G T ## 4335 1225 2055 6179\rsum(lfM)\r## [1] 13794\r","date":1580947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581008738,"objectID":"8d16e489e5ef46020884190c7e09cad2","permalink":"/post/chapter-1-exercise-setup/","publishdate":"2020-02-06T00:00:00Z","relpermalink":"/post/chapter-1-exercise-setup/","section":"post","summary":"Instructions on how to get started on Chapter 1, exercise 1.8.","tags":["Chapter 1"],"title":"Chapter 1 exercise setup","type":"post"},{"authors":["Brooke Anderson"],"categories":["quiz","Chapter 1"],"content":"\rThe vocabulary quiz will be live here during the start of the course.\nLoading…\r\r","date":1580947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580933164,"objectID":"3bb9cd5326655191aa8e191c887845d2","permalink":"/post/chapter-1-vocabulary-quiz/","publishdate":"2020-02-06T00:00:00Z","relpermalink":"/post/chapter-1-vocabulary-quiz/","section":"post","summary":"\rThe vocabulary quiz will be live here during the start of the course.\nLoading…\r\r","tags":["quiz","Chapter 1"],"title":"Chapter 1 vocabulary quiz","type":"post"},{"authors":["Brooke Anderson"],"categories":["instructions","vocabulary","blogdown","github"],"content":"\rAs one of your assignments for this class, you are responsible for creating a blog\rpost with all the vocabulary and definitions for one week of the course. This\rblog post will explain how you can create and publish that blog post on our\rcourse website.\nCreate the blog post\rUpdate your fork of the website\rYou should have already forked our website to add your details for the “Person”\rsection. You can use this same fork to add your blog post, but you should make sure\ryou get your fork up-to-date with the current version of the website before you do.\nA fork of a repository does not stay up-to-date with the original repository it\rcopied by itself. Instead, unless you update it, it will continue to be a snapshot\rof the original repository (plus any changes you’ve made to your copy) as of\rthe time when you forked it. If the original has made a lot of changes since you\rmade your fork, it might be very hard to make a clean pull request as there will\rbe (potentially) lots of conflicts because of changes made to the original. It’s\rconsidered polite to make sure that you’re working with an up-to-date fork of\ra repository if you want to make a pull request back to the original.\nTo update your fork of the original repository, open your “csu_msmb.Rproj” file\rto open our website’s R Project on your computer. This should open RStudio with\rthe website’s project open (check the top right corner of your RStudio window to\rconfirm—it should say “csu_msmb”).\nThere is a little blue gear symbol in the “Git” pane in RStudio. Click on the down\rarrow to the right of it and select “Shell…”. This should open a bash shell on\ryour computer. (If your computer uses Windows, there’s a chance that it might open\rsomething other than a bash shell. In that case, you can change your preferences in\rRStudio to reconfigure to always use a bash shell terminal when you ask for a shell\rfrom RStudio.)\nIn this shell, you need to run two git commands. First, you’ll add a remote branch\rto your repository. You already have one remote branch called “origin”—that’s the\rGitHub repository that you have in your account, which you forked from the original.\rNow you’ll add the original (the GitHub repository in my account) as another remote\rbranch. Each branch has its own name, and you can use that name to refer to it in later\rgit commands. The convention, if you add an original repository that you forked from\ras a remote, is to name that remote branch “upstream”. Run the following code in your\rbash shell to add the original GitHub repository as a remote branch with the name\r“upstream”:\ngit remote add upstream git@github.com:geanders/csu_msmb.git\rNow that you have added the original as a remote, you can pull in any commits that were\rmade to it since you originally forked it. There are a few ways you can do that, but one\rway to do it in one step is with git’s pull command. This fetches the changes and merges\rthem into your local version of the repository, all in one step. Run the following code in\ryour bash shell to do that:\ngit pull upstream master\rIdeally, all this will have worked seamlessly (if not, check with the faculty and we can\rhelp you troubleshoot). Close your bash shell and check your version\rof the “csu_msmb” project to see if it looks like it’s up-to-date with the original. You can\rgo to the “Commit” button in the “Git” tab, and there is a “History” selection in the window\rthat pops up. Look through that and make sure that you see recent commits to confirm that\ryour version is now up-to-date.\nFinally, this has updated your local version, but not your GitHub remote. Go ahead and use the\rgreen up arrow in RStudio’s “Git” pane to push your updated local version up to GitHub. Now\rboth your local (“master”) and remote (“origin”) branches should be up-to-date with our\roriginal version, so it will make it much easier to merge in your changes.\nIf you’d like to learn more about this process, there’s a really nice blog post\rhere.\n\rMaking a new blog post\rIn blogdown, each blog post is an RMarkdown document. The stuff at the very top\rof the file (the YAML with details like the title and author) will look a bit\rdifferent than plain RMarkdown files, but once you get into the body of the post,\ryou should find that the rules are very similar to RMarkdown.\nYou will be creating a blog post that will include a table with the vocabulary\rlist as well as a few other elements. There are a few ways you can add a new\rblog post file in blogdown. You’re welcome to use any method you’d like, but\rif you’re not sure where to start, this is one way.\nMake sure that you have RStudio open to the project for our course’s website. If you\rdo, you should see csu_msmb in the upper right hand corner of your RStudio session.\r(If not, go to File -\u0026gt; Open Project... and navigate through your file directory\rto your local version of our project directory and open the csu_msmb.RProj file there.)\nNext, you can use an RStudio “Addin” to make a new blog post using a nice user\rinterface. These Addins are all alternatives to things you could do with a\rfunction call in R, but the Addin often provides a more immediately\ruser-friendly interface for you to enter options. For example, the Addin for\rcreating a blog post does all the actions of a blogdown function called\rnew_post, but instead of needing to remember the parameter name to use for the\rauthor listing and the title and all that, you can just fill the information\rinto a nice form and go from there.\nTo find the new post Addin, look at the top of your RStudio session window. You\rshould see “Addins” with a down arrow beside it. Click on the down arrow. When\ryou do, you should see a “New Post” option. Select this option. A form should\rpop up with spaces for you to fill in the title, author, and some other details.\nFill this form out in the following way:\n\rTitle: This should be “Vocabulary for Chapter [x]”, but with “[x]” replaced\rwith your chapter number.\rAuthor: Make sure you put your name exactly as listed in the “People” section\rof the website. This will help the website generator connect this post with your\ruser profile, so when someone reads it they’ll get your picture and a link to\rfind out more about you at the end of the post.\rDate: This is where you put the publication date of your blog post, and it\rhas a pretty cool feature. Even if you write your blog post earlier, the post\rwill not be published on the blog until the date listed in this section. That means\rthat you can start writing your blog on one day but know that it won’t show up online\runtil later. It also means you can start work on your blog, but a half-finished\rdraft won’t show up online until you get to the publication date. For right now,\rset the current date in this section, so that your blog post will show up locally as\ryou work on it, but later you’ll actually change this date so that, when you submit\ryour pull request, your post won’t show up until the faculty have had the chance to\rsuggest some changes and for you to make any needed fixes.\rCategories and tags: For both the “Categories” and the “Tags”, be sure to\rinclude vocabulary and Chapter [x] (with [x] replaced with your chapter’s number).\rThese tags will let everyone on our website quickly find all the blog posts on your\rchapter or all the vocabulary lists.\rFormat: You have several choices for the type of file to use to write your\rblog post. Since we’re going to be using some R code to make the table look pretty,\ryou’ll need to pick one of the options that allows for R code chunks, so that rules\rout plain Markdown. I recommend that you use “.Rmd”.\r\rOnce you make these entries, click the button labeled “Done”. This creates an\rRMarkdown file for your blog post and opens it for you. Here’s an example of\rme doing this process if I were writing the vocabulary list for Chapter 16:\nYou can see, in the RMarkdown file that’s created and opened, that all these\rdetails end up getting inserted into the YAML at the top of the RMarkdown file.\rIf you ever need to change anything (like the date or the title), you can\rchange it here in the RMarkdown file. Do so carefully, though—YAML can be\rpretty picky about things like spacing and special characters (hyphens, for\rexample).\nIf you ever need to find this file later, all of the blog posts are saved in a\rspecial place in our project’s directory: in the content subdirectory, there’s\ra subdirectory called post that contains both the RMarkdown files used to\rwrite the posts and the output (an HTML file) that is created by the RMarkdown\reach time you save the file. You might notice that they all have long file\rnames—the file name for a blog post is a combinataion of its publication date\rand its “slug”, which is some abbreviation of the original title. If you really\rwant, you can change what the slug will be when you first create the blog post,\rbut I don’t think you really need to.\n\rWriting in the blog post\r\r\rvia GIPHY\rWithin the body of the blog post RMarkdown file (in other words, below the\r--- that marks the end of the YAML section), you can write the blog post\rjust as you would any RMarkdown document. This means that you can use\rthings like ** to mark bold text, * for italics, and #, ##, etc., for\rsection headings.\nIt also means that you can insert chunks of R code that will run and add\rtheir output within the post. Unlike in regular RMarkdown, you usually won’t\rhave to press the Knit button to knit the document. Instead, the blog post\rshould re-knit every time you save the file. You can check to see by looking\rat the Viewer pane to look at the current version of the site (if it doesn’t\rshow the site automatically, load the blogdown package and then run\rserve_site).\nIf you have not worked much with RMarkdown before, you might want to check\rout some references on how it works. There are several great articles on the\rRMarkdown website that can help.\nIn your blog post, go ahead and draft a first paragraph that describes the\rkey concepts covered in the chapter. Also, create third-level section headings\r(i.e., use ### to mark the section heading) for “Sources consulted or cited”\rand “Practice”. Save your blog post file and check to see if these changes\rhave been made in the version of the website in your Viewer pane!\n\r\rCreate the vocabulary list\rNow, for the content of your post. You’ll be creating a vocabulary list, as well\ras embedding a Quizlet practice app, so that your classmates can learn the\rvocabulary for the chapter. This list will be what everyone is responsible for\rin the weekly vocabulary quiz.\nYou can see an example of a vocabulary blog post for\rChapter 1. You can use\rthis as a template for your own post.\nIdentify the vocabulary terms you need to define\rFirst, you will need to decide which words from the chapter to define. We expect\rthat you will include all the bolded terms for your chapter.\rHere are some guidelines for deciding on the vocabulary terms to define for your\rchapter:\n\rYou should include all words in the chapter that are given in bold. Be sure to\rlook for bolded terms in the sidenotes and end-of-chapter exercises, too! Occassionally,\rthe authors use bold for subheadings (see the “Why R and Bioconductor?” section in\rthe Introduction or the “Summary of this chapter” section of Chapter 1). These\rsubheadings do not need to be included in the vocabulary list for the chapter.\rIf you find one or more common synonyms for a term, you can include that with\rthe term in the list (e.g., “variability / spread / dispersion”).\rFeel free to change a term from singular to plural or vice versa if it helps you\rin writing the term’s definition. Similarly, if the bolded term does not include\rall the words that would be helpful (e.g., the bolded term is “sufficient”, but\rthe term of interest is “sufficient statistic”), you can add a word or two to the\rbolded term.\rThe bolded terms in the book tend to favor statistical terms over biological ones.\rIf there are some biological terms you needed to look up when you read the chapter,\ror that you think some people in the class might not know,\rfeel free to add them to your vocabulary list.\r\r\rCreate a .tsv file with terms and definitions\rWhile you could directly add the vocabulary into an RMarkdown table, we are asking you\rto save it into a plain text .tsv file, which will then be read into the RMarkdown\rdocument to form a table. We doing this because it creates a few advantages. First,\rif we have the vocabulary list in a dataframe (which we get when we read it in from\ra plain text file), we can use some cool R packages to format the table nicely, without\rhaving to learn loads of new Markdown or HTML formatting tricks. Second, we want to also\ruse the vocabulary list as input to a Quizlet list, which\rwill let us embed a practice app with flashcards and quizzes. One of the easiest ways\rto create a Quizlet list is to copy in vocabulary list directly in the tsv format,\rso this approach makes that secondary use easy.\nIn our website’s repository, there is a special subdirectory for saving vocabulary list\r.tsv files, with one for each chapter. In the Project directory, go to content -\u0026gt;\rpost -\u0026gt; vocab_list. This is where you want to save the .tsv file for your chapter.\nTo create the file, in RStudio go to the “File” tab in the menu at the top and select\r“New File” -\u0026gt; “Text File”. This will open a file in RStudio in plain text format. Save the\rfile as “chapter_[x].tsv” (but replace “[x]” with your chapter number). Make sure you save\rit in the “vocab_list” subdirectory of the project with the rest of the vocabulary list files.\nNow write your vocabulary terms and definitions in this .tsv. This file extension stands\rfor “tab-separated”, so to format the file correctly, you should:\n\rPut each term / definition pair on its own line. Because some terms will be long, they\rmay visibly “wrap” in the text file you have open, but as long as you don’t press the\r“Return” key, they should still be on one line of the file. To doublecheck, you may want\rto make sure that you have line-numbering on in RStudio and make sure that only one line\rnumber is listed for each term on the left hand side of the file.\rPress the “Tab” key to add a tab between the term and definition on each line. This should\rbe the only place you have tabs in the file. R will look for tabs to figure out where to\rsplit between vocabulary terms and there definitions (as will Quizlet when you copy the\rterms into the list there). Sometimes it won’t look like the tab’s added a lot of space,\rbut that’s no problem—the computer can see it even if you can’t!\rDon’t put any header information at the start of the file. Just start directly with your\rfirst vocabulary term.\r\rIf you’d like to see an example, check out the “chapter_1.tsv” file in the “vocab_lists”\rsubdirectory. This is the file that serves as input for the Chapter 1 vocabulary list\rblog post.\nHere are some guidelines for writing your definitions:\n\rIt is fine to use wording from the chapter text or to use wording directly from\rother websites or sources. However, you must include a list of any of the\rsources that you used to write your definitions at the bottom of the vocabulary\rblog post. Further, if you are using sources besides the course textbook, make sure\rthat the definition is appropriate in the context of our course. Often, words will\rhave a number of different definitions across different disciplines. Try to use\rmore formal sources (e.g., textbooks, other published books) rather than less\rformal websites to find definitions whenever possible. See the\rChapter 1 vocabulary list\rfor an example of what we expect for using and listing references.\rIf a vocabulary term was defined in a previous chapter’s vocabulary list, feel free\rto reuse the definition.\rOur library has excellent resources that you can use to help write your definitions,\rincluding textbooks and dictionaries specific to biology and statistics.\r\r\rAdding R code to show the list in the post\rI’ve written up some R code that will read in the vocabulary list and make it\rinto a nicely formatted table in the HTML version of the blog post. You can\rre-use this R code in your post, you’ll just need to change the name of the\rinput file to the one for your chapter’s file.\nThis R code uses a few R packages beyond the base R code. If you haven’t installed\rthese packages yet, you’ll need to before the code will run. You’ll need to install:\n\rknitr\rdplyr\rreadr\rkableExtra\r\rOnce you have these, below your paragraph summarizing the chapter’s theme, write:\n\r“The vocabulary words for Chapter [x] are:”\n\r(but with your chapter’s number) and then paste in the following code and change\rchapter_1.tsv in the code to the correct file name for the .tsv file you created\rfor your chapter’s vocabulary.\n```{r echo = FALSE, message = FALSE, warning = FALSE}\r# Load packages\rlibrary(dplyr)\rlibrary(readr)\rlibrary(knitr)\rlibrary(kableExtra)\r# Read in vocabulary from tsv into a dataframe\r# This is where you'll need to replace the file name with your own\rvocab % kable(align = c(\"rl\"), col.names = c(\"\", \"\")) %% kable_styling(bootstrap_options = c(\"striped\", \"hover\",\r\"condensed\")) %% column_spec(1, bold = T, border_right = T) %%\rcolumn_spec(2, width = \"30em\")\r```\rThis code reads in the data from your .tsv file and then formats it in a nice way.\rIf you’d like to understand it better, try commenting out some lines and see how it\rchanges the output. One of my favorite piece of this code, one that I think might\rcome in useful for you later, is column_spec(2, width = \"30em\"). This sets the\rwidth of one of the columns to be 30 ems (the width of the letter “m” in whatever\rfont you’re using). By setting the width, the table won’t automatically expand to\rfit the text you put in the column onto one row. Instead, it will allow the text\rto “wrap”, going onto separate lines if the definition entry is long enough.\nIf you want to find out more about creating really fancy tables from RMarkdown,\rcheck out the documentation on the kableExtra package. What you can do (and how)\ris different, depending on whether you’re outputting to a pdf or a HTML file, so there’s\rseparate documentation for each.\nOnce you add this code in, I’ve found that you actually do need to press the Knit\rbutton sometimes. If you don’t see your list when you save your file, or if it doesn’t\rupdate properly as you make changes to your file, try knitting with the Knit button\rand that should help.\n\rCreating and embedding a Quizlet app\rThe last piece of the blog post is the practice section. For this, you’ll create\ra vocabulary list on Quizlet, which you can then embed in the\rblog post, so the other students can practice right on our site.\nYou’ll need to sign up for a Quizlet account first. The free account is fine.\nNext, create a new vocabulary list. There’s a “Create” button for making new lists\ron the main page. While you can add vocabulary by hand, you can also post in a\rwhole list if it’s in a tab-separated or comma-separated format. Copy in the contents\rof your vocabulary list .tsv file. You can preview the terms lower on the page\ronce you do, to make sure that all the terms and definitions came in correctly.\rIf everything looks good, click on the buttons for “Import” and then “Create”.\nThis will create your list and take you to a page where you can try out your\rflashcards. On this page, there’s also a button with three dots. If you click on\rthis, there’s a choice of “Embed”. When you embed HTML content, you are\rinserting an application from one website within another one. Embedding is a\rreally fun trick for enriching blog posts and other RMarkdown documents that\rare rendered to HTML. For example, you can also embed Shiny apps, YouTube\rvideos, and Google maps in your RMarkdown using the same process we’ll use here.\nWhen you select “Embed”, a pop-up window will open with some HTML code. Copy\rthis and then paste it in the “Practice” section of your vocabulary blog post.\rBe sure to leave a blank line above and below the text you paste. When you\rlook at your blog post in a web browser now, you should see the practice\rflashcards embedded in the “Practice” section.\n\r\rSubmit the post\r\r\rvia GIPHY\rSo far, you’ve made these changes to your local copy of our website’s\rrepository. To submit the changes to us, you’ll need to push your changes\rto your remote version of the repository (the one in your GitHub account)\rand then submit a pull request to us for us to pull those changes into the\roriginal website repository (the one in my GitHub account). This process\rshould feel pretty familiar—it’s pretty much what you did to submit your\rchanges to your profile information for the website on the first day of class.\nAs with other steps, there are several ways you can do this, and if you have an\ridea of how to get it done, any way is fine. If you don’t know where to start,\rthough, you can follow along in this section for one way to do it.\nPushing your changes to your remote repo\rFirst, you’ll need to get any changes you’ve made from your local\rrepository up to your remote version on GitHub.\nFirst, commit any changes that you’ve made through the Git window in your\rRStudio session. This will record the changes you’ve made in the git record\rfor your local repository.\nNext, you’ll need to push these commits to the remote repository, to send\rthese changes to GitHub. In the Git window in RStudio, there’s a green up\rbutton. Push that. It should send all your changes up to your GitHub version of\rthe repository. To check, go online to your GitHub account and look through\ryour repositories for your fork of “csu_msmb”. Click on “Commits” to see a\rhistory of the commits to the repository—your latest ones should be at the top\rof the list.\n\rSubmitting a pull request to the original repo\rAt this point, you’ve made changes, checked them, and pushed them to your GitHub version of\rthe repository. Remember, though, that you forked the repository from our original one, and\rso you’ve been working with a copy of the repository this whole time, rather than changing\rour original version.\nTo get your changes incorporated into our original version, you’ll need to request that we\rpull your changes into the original repository. To do this, you can submit a pull request\rthrough GitHub. Go to the main page for your fork of the GitHub repository and look for\ra button that says “New pull request”. When you click this, it will walk you through making\ra pull request. You’ll have a space to write a message describing the changes you’re\rrecommending in the pull request.\nIf you’d like more details on this information, GitHub has help documentation\ron pull\rrequests.\n\r\rEdit and re-submit the post based on faculty feedback\rThe other faculty members and I will get a notice when you submit your pull request. We’ll\rtake a look and will probably have some suggestions for the wording of some of the\rvocabulary terms. We’ll give you some feedback through the pull request page, and then\rwe’ll work together to get the list finalized before it’s published for the rest of the\rclass.\n\r","date":1580774400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580751472,"objectID":"b6a63bb5b3e96b091185e33290708428","permalink":"/post/creating-a-vocabulary-list-blog-post/","publishdate":"2020-02-04T00:00:00Z","relpermalink":"/post/creating-a-vocabulary-list-blog-post/","section":"post","summary":"As one of your assignments for this class, you are responsible for creating a blog\rpost with all the vocabulary and definitions for one week of the course. This\rblog post will explain how you can create and publish that blog post on our\rcourse website.\nCreate the blog post\rUpdate your fork of the website\rYou should have already forked our website to add your details for the “Person”\rsection.","tags":["instructions","vocabulary","blogdown","github"],"title":"How to create a vocabulary list blog post","type":"post"},{"authors":["Brooke Anderson"],"categories":["blogdown","github","instructions"],"content":"\rOne goal of this course is to continue developing your data science programming\rskills. This will include plenty of work on R programming, but also more to\rhelp you learn tools for reproducible research, like\rRMarkdown and git.\nWe will be using our course website as a collaboration tool during this course.\rThis website was created using blogdown,\rwhich allows you to create and update a blogging website with R and RStudio. We\rare using a GitHub repository to share\rall the code for this website and serving the site using Netlify.\nDuring this course, you will have two graded products that you will need to\rsubmit as blog posts to our site. One will be a glossary of vocabulary terms for\rone chapter of the book, listing key words and their definitions for the\rchapter. The second will be the “official” version of one week’s in-course exercise.\nTo help get you up to speed with using blogdown, GitHub, and RMarkdown with\rour site, we’ll start by having you update your profile details for our website.\rWe’ll also use this to give you all a chance to introduce yourselves to each\rother and to us. This post covers the details for how to do that.\nRequired set-up\rThis exercise, and this course as a whole, requires a certain set-up\ron your computer:\nR installed on your laptop\rRStudio installed on your laptop\rgit installed on your laptop\rYour own GitHub account\r\rIf you already have all this set-up, you can skip to the next section.\rOtherwise, this section has details on completing this set up.\nInstall R on your laptop\rYou can install R from the Comprehensive R Archive Network (CRAN).\rSearch for the version appropriate for your computer’s operating system.\nIf you already have R installed, check your version number. If it’s older than\rsix months or so, you should probably update your version for the class. You can\ruse the sessionInfo() function to find out details about your current R\rsession, including the version of R you’re currently running:\nsessionInfo()\r## R version 3.6.3 (2020-02-29)\r## Platform: x86_64-apple-darwin15.6.0 (64-bit)\r## Running under: macOS Mojave 10.14.6\r## ## Matrix products: default\r## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib\r## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib\r## ## locale:\r## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\r## ## attached base packages:\r## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached):\r## [1] compiler_3.6.3 magrittr_1.5 bookdown_0.18 tools_3.6.3 ## [5] htmltools_0.4.0 yaml_2.2.1 Rcpp_1.0.3 stringi_1.4.6 ## [9] rmarkdown_2.1 blogdown_0.18 knitr_1.28 stringr_1.4.0 ## [13] digest_0.6.25 xfun_0.12 rlang_0.4.5 evaluate_0.14\rBased on the return from this call, for example, I can tell that I have\rR version 3.6.3 (2020-02-29).\n\rInstall RStudio on your laptop\rYou can download RStudio directly\rfrom their website. The free\rDesktop version will work great for this course. If your version of RStudio is\rmore than a year old, you should probably update it for this course. To check\ryour version of R Studio, open R Studio, go to the “RStudio” tab at the top,\rand click on “About RStudio”.\n\rInstall git on your laptop\rThe software git is version control software, which will help you record and\rtrack changes that you’ve made to code and other plain text documents.\nIt’s free to download. Go to https://git-scm.com/downloads and select the version\rfor your operating system. For this software, you’re probably okay if you\rdownloaded it a little while ago (although if more than two years or so, you might\rwant to update).\n\rGet a GitHub account\rYou will need a (free) GitHub account for this course. You can sign up for one\r(if you don’t already have one) at https://github.com/. While there are some\rfancier paid plans available, the free account will work great for this class.\nWhen you sign up, you’ll get to choose a GitHub handle. You might want to make\rthis something that will be easy for people to remember. For example, if your\rname is still available, that would be a great option. This handle will form\rpart of the address to all of your GitHub repositories, so it is convenient if\rit is easy for people you work with to remember (mine, unfortunately, is not!).\n\r\rAbout blogdown\rThe blogdown package is an R package created by Yihui Xie that allows you\rto use R and RStudio to create and update your own webpage with a blog. The\rappeal of being able to do this with R is that you can write blog posts\rusing RMarkdown, so you can include executable R code in each post.\nblogdown creates your site using the Hugo framework.\rHugo is software that can build static websites (i.e., ones that can be\rserved to viewers without needing database backends or other fancy things).\rPeople have created different templates for Hugo-generated websites, and these\rtemplates provide the structure and framework, while you can adapt the content.\nThis means that our website (which is, essentially, a collection of files in\ra directory written in a form that a web browser can convert to a pretty\rwebsite) includes a lot of files and code that come straight from a template\rthat someone else wrote, and then places here and there where we can add or\rchange the files to make the website ours.\nOne of the ways that you can change the website is to add posts. You’ll be\rdoing this later in the course by contributing two blog posts of your own,\rone on the vocabulary for a chapter and one with the “official” version of\rthe exercise for a chapter. We’ll cover more on how to add a blog post later\rin the course.\nThe other way that you can change the website is to change some of its\r“front page” data. The website has a section on “People”, with the\rprofiles of everyone in the class. The information shown in this section\ris all saved in plain text files in our website’s file directory. Today,\ryou’ll change you details in the file dedicated to you and then send those\rchanges back to us so we can update the online version of the site.\nYou will need to install two pieces of software to work on our\rwebsite. First, you’ll need the R package blogdown. You can install\rthis package in the normal way, using install.packages:\ninstall.packages(\u0026quot;blogdown\u0026quot;)\rOnce you have blogdown, you can install the Hugo software using a function\rin the blogdown package, install_hugo:\nlibrary(blogdown)\rinstall_hugo()\rFor both these installations, your computer will need to be online or you’ll\rget an error.\n\rGetting a fork of our repository on your computer\rAll our websites files are posted in a GitHub repository at https://github.com/geanders/csu_msmb. With this (or any) GitHub repository,\ryou can suggest changes by forking the repository, cloning the fork to\ryour computer, making and committing the changes, pushing those commits\rback up to your fork of the repository on GitHub, and then submitting\ra pull request.\nForking a GitHub repository\r\r\rvia GIPHY\rWhen you fork a GitHub repository, you get a copy of that repository that you\rcan play around with and change yourself, without it affecting the original\rrepository. It’s essentially just copying the whole repository, with all its\rfiles, into a repository on your GitHub account.\nThe only thing that makes it different from a plain copy (and what makes it\rreally powerful in some cases) is that, if you decide that your changes might\rmake the original repository better, you can submit a pull request. This\rrequests that the owners of the original repository update it to incorporate the\rchanges you’ve made on your fork of the repository. The original authors can\rreview each of the commits you’ve made, so they can even cherry-pick your\rchanges if they want.\nGitHub also lets the original authors see if there are any merge conflicts\rcreated from changes that they’ve made to the original repository since you\rforked it. This can let the original authors see how hard it will be to\rincorporate all of your changes in the forked version with their version of the\rrepository.\nTo fork the repository with our course’s website materials, all you’ll need to\rdo is go to our GitHub repository for the course\rwebsite (while you’re signed in to\rGitHub) and click on the “Fork” button towards the right of the page. Now go and\rcheck in the “Repositories” section of your own GitHub account—you should see that\ryou now have a forked copy of the “csu_msmb” repository.\nIn this exercise, you’ll work with the fork of the repository, and then once you’ve\rmade your changes, you submit a pull request, so that we can get your changes\rback into the main webpage.\nIf you need more help on how to fork a repository, GitHub has a help\rpage\ron the topic that might be useful.\n\rCloning the fork to your computer\rNext, you’ll want to get a copy of your forked repository onto your own computer,\rwhere you can work with it, make changes, and preview the website with your\rupdates.\nTo do this, you’ll clone your fork of the repository onto your computer.\rThe version of the repository on GitHub is called the remote branch of the\rrepository and the version you get on your computer once you’ve cloned it is\rthe local branch. By cloning (instead of just downloading), you’ll maintain\ra connection between the remote and local versions through git, which will allow\ryou to push changes that you make and commit on your own computer up to the\rremote branch on GitHub.\nGo to GitHub, make sure you are logged into your account, and navigate to your\rforked version of the repository for this class. There should be a button to the\rright of the page that says “Clone or download” (you may need to scroll down to\rfind it).\nWhen you click on this button, it will give you a choice between\r“SSH” and “HTTPS” for the protocol to use to connect your local and remote branches\rof the repository. You’re welcome to try either, but I usually (on a Mac) have\rbetter luck with “SSH”. Occasionally, people running Windows in my courses have\rhad better luck with “HTTPS”, although for most folks “SSH” seems to work fine.\rOnce you choose which protocol to use, you can copy the snippet of code that is\rgiven in the pop-up.\nNext, you’ll run this code from a bash shell on your own computer to clone the\rrepository. You first will need to open a shell. If you’re on a Mac, you can do that\rwith the “Terminal” application. With Windows, you’ll probably need to use the\rbash shell that comes with the Windows version of git. Search your programs for\r“bash” or “git bash” and see if you see something that looks promising.\nOnce you open a shell, you’ll see a command prompt, like this:\nusername$\rYou can type shell commands here and then press “Return” to run them. You\rshould first move into the directory where you want to clone the repository.\rYour “Desktop” might be a good place for it for now (unless you have some\rorganization you use for course-related files). The cd shell command lets you\r“change directory”. If you don’t put anything after cd, it will change to\ryour home directory. Otherwise, it will move to the directory you specify.\rFor example, if the “Desktop” directory is a subdirectory of my home directory,\rI could move into it by running:\ncd Desktop\rIf you have not use shell commands much before and are having any problems\rnavigating to the directory you’d like, let us know in class, and one of us can\rhelp you.\nOnce you are in this directory, you’ll paste git clone followed by the command\ryou copied from the “Clone or download” button on GitHub. It will probably look\rsomething like this (but with your GitHub handle in place of “geanders”):\ngit clone git@github.com:geanders/csu_msmb.git\rWhen you run this, you may have to put in your GitHub username and password. You\rmay also get some questions about whether you really want to download the\rrepository (you do). If everything’s successful, you should see that there’s a new\rdirectory called “csu_msmb” in which directory you decided to put it (“Desktop”,\rfor example).\nThis directory has a special file in it that makes it an R Project—a special\rversion of a file directory with some extra structure and saved preferences. Make\rsure that you open the project as a whole when you work on it in R Studio, rather than\ropening just by clicking on one of the files. To do this, you can go in R Studio\rto \"File\" -\u0026gt; \"Open Project...\" and then navigate through your file directory\rto the “csu_msmb” directory you just cloned.\nIf you need more help, GitHub has a help\rpage\rwith more on how to clone a repository from GitHub to your own computer.\n\rChanging and committing in RStudio\rWhen you have R Studio open to an R Project that is using git version control,\rR Studio will include a “Git” pane. You can use this pane to commit changes you\rmake to files in the repository, write messages explaining those commits, and\rpush your changes to your remote branch of the repository on GitHub.\nWhen you commit a change, that change is written into a log of every change made\rto the files in the repository. You can later look through these commits, so you’ll\rwant the commit messages to make sense when you read them in the future. When you’re\rcollaborating with others, the commit messages will help you see what each other\rare doing.\nWhen you first commit a change, the commit is only saved in your local branch.\rTo send it up to the remote branch of the repository on GitHub, you’ll need to\rpush those commits. Once you push your local commits, your GitHub repository\rshould exactly mirror your local repository.\nAs soon as you make a change to a file in the repository that’s being track by\rgit, that file will show up in the Git pane, with a little check box beside it.\rWhen you’re ready to commit a change, click on the “Commit” button on the top left\rof the Git pane. This will open a pop-up box.\nIn this box, click the check boxes for all the changed files on the left you’d\rlike to include in the commit. Then write a short commit message, describing\rthe changes you’ve made. You should try to fit it all in the first line of the\r“commit message” window. If you can’t, write a short description in the first line,\rskip a line, and then you can write as much as you want.\nOnce you’ve written your commit message, click on the “commit” button. This will\rrecord this commit. To check that it has, you can go to the “History” tab and\rmake sure the commit shows up as the last thing in your history.\n\r\rUpdating your profile details\rRendering blogdown websites in RStudio\rOnce you’ve opened the R Project with our website, you can use the\rblogdown package to serve the website. This will only update and\rshow the website on your computer (not change our main website online),\rbut it lets you check that everything’s working and preview what the\rsite will look like online.\nRStudio’s “Viewer” pane can work as a web browser. This means that it can\rshow our website. When you have opened the R Project with the cloned\rrepository of our website (“csu_msmb”), try running the following in your\rR console to render the site:\nlibrary(blogdown)\rserve_site()\rIf everything worked, you should be able to see a version of the website in your\rRStudio “Viewer” pane. If you’d like to see it in your usual web browser, click\ron the “Show in new window” button on the top left of the “Viewer” pane (this\rlooks like a little rectangle with an arrow on it). This will open the website\rin your default web browser.\nTake a look at the web address when you do—it should start with 127.0.0.1.\rThis is a loopback address—an IP address that refers back to your local\rcomputer (localhost), rather than an outside web servers. Anytime you’re\rbuilding a website and checking it locally, you’ll see this in the web address\rwhen you open the site in a web browser. (You can even get T shirts with “There’s no place like\r127.0.0.1”).\nAs you work through the next parts of the exercise, the rendered website in the\rViewer pane should update every time you save your changes to files in the\rwebsite. If you have the website open in your default browser, too, you might\rwant to refresh the site with the normal “Refresh” button for your browser.\rIf things ever seem like they’ve gotten out of sink, you can always re-run\rserve_site().\n\rNavigating the website’s file directory to find your profile\rWe all have our own author profile in a subdirectory within the\rwebsite’s files. To find yours, go to the “content” subdirectory\rof the website files and then the “authors” subdirectory within that.\rYou should see a subdirectory there with your name. Click on that\rand you’ll see the two files that make up your author profile,\r\"_index.md\" and “avatar.jpg”.\n\rUpdating your information in \"_index.md\"\rYour details are all given in the \"_index.md\" file in your author subdirectory.\rTo update your details on the website, you’ll need to change your details in this\rfile.\nThe file is written in a Markup language called YAML. If you’ve used RMarkdown\rbefore, you might recognize this syntax from the information that goes at the\rvery top of each RMarkdown file.\nIn your \"_index.md\" file, anywhere there is a placeholder, like “[Year]”\ror “[Institution]”, replace the placeholder with your own information.\nBe very careful when changing things like spaces and hyphens in the structure,\ras YAML is based on parsing these elements. As with any Markup language, as\ryou are learning it, it’s best to try to render the final document often as\ryou make changes, so you can make sure the changes make it through like you want\rand so you can catch any problems quickly.\nMake sure you change the following sections:\n\rbio:\reducation:\remail:\rinterests:\rname:\rorganizations:\r\rSome of the sections in social: are commented out, including the information\rfor buttons for GitHub, GoogleScholar, and Twitter. If you have accounts through\rany of these services, you can add these buttons with your updated information.\rJust delete the # at the beginning of all lines in that section and then\rchange the handle or web address information so that it links to your\raccount for that service. In this section, also update your email address, with\rmailto: at the beginning, for the email icon.\nThe very bottom of the file, under the ---, provides space for you to write\ra paragraph summarizing who you are and your academic / research interests.\nBe sure to save the file after you’ve made all your changes.\nFor an example of a completed \"_index.md\" file, you can see mine here.\n\rUpdating your avatar picture\rThere’s also a place in your author profile directory to include a photo to\rrepresent yourself. To change from the default (the blue guy), replace the\r“avatar.jpg” file in your author profile directory with the JPG of your choice,\rand use the same file name (“avatar.jpg”).\nIt would be helpful for you to use a photo of yourself, since that will help\rus put names with faces, but if you don’t have one or would prefer\rnot to use your own photo, feel free to pick any photo (for which you have\rappropriate permissions) to use.\nYou might need to crop your photo some to get it to show up in the circle on\rthe website correctly. Try with your uncropped picture once, check the website\rin the RStudio Viewer pane to see how it looks, and then if it doesn’t work,\rplay around with cropping it until you’re happy.\n\r\rSubmitting your updates\rPushing the commits back to GitHub\rWhen you are ready to push all the changes you’ve committed to your local branch, you can\rdo this from the Git pane in R Studio. In this pane, there are two arrows: a green up\rarrow and a blue down arrow. Click on the green up arrow to push the commits from your\rcomputer (the local branch) to GitHub (the remote branch). Visit your GitHub page for\rthe repository (or refresh it if you already had it open) and check if your changes\rhave successfully been pushed to the remote branch.\nIf you haven’t created an SSH key and shared it with GitHub, you may be asked\rfor your GitHub password every time you try to push. This will get to be a pain,\rso you’ll probably want to set up an SSH key. For more on how to do this (as\rwell as other help with using RStudio with version control), check out\rRStudio’s help documentation on the\rtopic.\n\rRequesting that we pull your changes\r\r\rvia GIPHY\rAt this point, you’ve made changes, checked them, and pushed them to your GitHub version of\rthe repository. Remember, though, that you forked the repository from our original one, and\rso you’ve been working with a copy of the repository this whole time, rather than changing\rour original version.\nTo get your changes incorporated into our original version, you’ll need to request that we\rpull your changes into the original repository. To do this, you can submit a pull request\rthrough GitHub. Go to the main page for your fork of the GitHub repository and look for\ra button that says “New pull request”. When you click this, it will walk you through making\ra pull request. You’ll have a space to write a message describing the changes you’re\rrecommending in the pull request.\nIf you’d like more details on this information, GitHub has help documentation\ron pull\rrequests.\n\r\r","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578952088,"objectID":"aaf734a217c669a7993ece1eb560bb61","permalink":"/post/add-profile-details/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/post/add-profile-details/","section":"post","summary":"One goal of this course is to continue developing your data science programming\rskills. This will include plenty of work on R programming, but also more to\rhelp you learn tools for reproducible research, like\rRMarkdown and git.\nWe will be using our course website as a collaboration tool during this course.\rThis website was created using blogdown,\rwhich allows you to create and update a blogging website with R and RStudio.","tags":["blogdown","github","instructions"],"title":"How to add your profile details to our course website","type":"post"},{"authors":["Brooke Anderson"],"categories":["vocabulary","Chapter 1"],"content":"\r\rChapter 1 covers generative modeling for discrete data. It introduces a number of terms covering\rprobablity and statistical modeling, as well as a few biological terms. The vocabulary words for\rChapter 1 are:\n\r\r\r\r\r\r\r\rprobability model\r\rA mathematical description of the possible outcomes of an experiment and the probability of each of those outcomes.\r\r\r\rvector\r\rIn programming, a one-dimensional array of data, all with the same data type.\r\r\r\rdiscrete event\r\rIn statistics, an event that can take a finite or countable number of values (e.g., number of deaths in a community by day).\r\r\r\rcategorical variable\r\rA variable that can belong to one of a finite set of levels.\r\r\r\rlevels\r\rIn the context of a categorical variable, the set of values to which the variable can be assigned.\r\r\r\rfactor\r\rIn the context of statistical programming, a data type that can take one of a limited number of possible values (e.g., sex, nationality).\r\r\r\rexchangeable\r\rA property of a vector of random variables that implies the order in which the variables appear in the vector doesn’t matter.\r\r\r\rsufficient statistic\r\rA (summary) statistic that contains all the information about the model parameters that is in the original, uncondensed form of the data.\r\r\r\rBernoulli distribution\r\rA probability distribution describing a random variable that can take on two possible outcomes (e.g., win / loss).\r\r\r\rparameter\r\rA numerical value that describes a population.\r\r\r\rcomplementary\r\rA description of two events who are mutually exclusive and whose probabilities sum to one (i.e., either one event or the other is guaranteed to happen, but not both).\r\r\r\rbinomial random variable\r\rA variable whose values occur according to a binomial probability distribution.\r\r\r\rprobability mass distribution\r\rA function giving the probability that a discrete random variable is equal to a given value.\r\r\r\rPoisson distribution\r\rA probability distribution for count data that has support on the non-negative integers. This distribution is also used to approximate a binomial distribution when the probability of success is small and the number of trials is large.\r\r\r\repitope / antigen determinent\r\rSite on a macromolecular antigen to which an antibody binds. This is the part of an antigen that is recognized by the immune system.\r\r\r\rEnzyme-linked immunosorbent assay (ELISA)\r\rAn assay that is used to detect specific epitopes at different positions along a protein.\r\r\r\rconditional on\r\rGiven\r\r\r\rcumulative distribution function\r\rA function giving the probability that a random variable is less than any specified value.\r\r\r\rextreme value analysis\r\rAnalysis focused on the behavior of the very large or the very small outcomes of a random distribution, allowing an exploration of the probability of rare events.\r\r\r\rrare event\r\rSomething that occurs with a very low probability.\r\r\r\rrank statistic\r\rA data vector sorted least to greatest.\r\r\r\rMonte Carlo method\r\rA method that uses computer simulation from a generative model to determine probabilities of events.\r\r\r\rprobability or generative modeling\r\rA method of modeling where all the parameters are known and the mathematical theory allows us to work by deduction.\r\r\r\rdeduction\r\rA top-down method of reasoning, starting from a theory or principle rather than from data.\r\r\r\rstatistical modeling\r\rA method of modeling where the distribution of the data is not known.\r\r\r\rfit\r\rIn the context of statistical modeling, estimating the parameters of a model based on observed data.\r\r\r\rmultinomial\r\rA generalization of the binomial distribution to cases where there are a finite set of possible outcomes (e.g., a roll of a die).\r\r\r\rpower / true positive rate\r\rThe probability of detecting something if it is there.\r\r\r\rnull hypothesis\r\rOften, a hypothesis of “no association” that is used as a counterpart to a more interesting alternative hypothesis in hypothesis testing.\r\r\r\rmatrix\r\rIn programming, a two-dimensional array of data, all with the same data type.\r\r\r\rexpected value\r\rThe average (mean) value of a random variable.\r\r\r\rvariability / spread / dispersion\r\rIn statistics, the amount by which a set of observations deviate from their mean.\r\r\r\rstatistic\r\rA numerical characteristic of a sample and known constants (i.e. no unknown parameters).\r\r\r\rnull distribution\r\rThe probability distribution under the null hypothesis.\r\r\r\ralternative\r\rIn the context of a generating process and hypothesis testing, the generating process that is considered in comparison to the generating process under the null hypothesis.\r\r\r\rchi-squared distribution\r\rA distribution on the non-negative real numbers that is often used in assessing goodness-of-fit (e.g. models fit to contingency tables).\r\r\r\rp-value\r\rThe probability of seeing the observed data or something more extreme under the generative model associated with the null hypothesis.\r\r\r\rprobability density function\r\rA function giving the relative likelihood that a continuous random variable is equal to a given value. When this function is integrated over the sample space, it equals 1.\r\r\r\rdefault\r\rIn the context of arguments to an R function, the value that is used if no custom value is specified.\r\r\r\rC. elegans genome nucleotide frequency\r\rHow often adenine, cytosine, guanine, and thymine occur in the DNA of a roundwork often used in scientific research.\r\r\r\rBioconductor\r\rOpen-source software that provides contributed programs for bioinformatic data analysis.\r\r\r\rcodon\r\rA three-nucleotide sequence that specifies the amino acid to be created next (or to start or stop synthesis).\r\r\r\rDNA read\r\rAn inferred sequence of base pairs for a single DNA fragment, based on sequencing.\r\r\r\rnucleotide\r\rIn the context of DNA, one of four compounds (adenine (A); cytosince (C); guanine (G); and tymine (T)) that make up the basic information unit.\r\r\r\rgenome\r\rAn organism’s complete set of DNA, including all of its genes.\r\r\r\rreplication cycle\r\rIn biology, the process that begins with the infection of a host cell by a virus and ends with the release of mature progeny virus particles.\r\r\r\rpoint mutation\r\rA change, addition, or deletion of a single nucleotide in a gene sequence.\r\r\r\rgenotype\r\rThe genetic make-up of an individual’s cells, including how the individual’s genetic make-up differs from others’.\r\r\r\rdiploid\r\rHaving genetic material in two complete sets of chromosomes, from two parents.\r\r\r\rprotein\r\rA compound made up of amino acids; one of the four types of macromolecules that make up living organisms.\r\r\r\rantibody\r\rA type of protein made by certain white blood cells in response to an antigen.\r\r\r\rantigen\r\rA foreign substance in the body to which the immune system reacts.\r\r\r\r\rSources consulted or cited\rSome of the definitions above are based in part or whole on listed definitions\rin the following sources.\n\rHolmes and Huber, 2019. Modern Statistics for Modern Biology. Cambridge University Press,\rCambridge, United Kingdom.\rEveritt and Skrondal, 2010. The Cambridge Dictionary of Statistics (Fourth Edition). Cambridge University Press, Cambridge, United Kingdom.\rBioconductor: Open Source Software for Bioinformatics. https://www.bioconductor.org/\rWikipedia: The Free Encyclopedia. https://en.wikipedia.org/wiki/Main_Page\rNIH Genetics Home Reference. https://ghr.nlm.nih.gov/\rNCI Dictionary of Cancer Terms. https://www.cancer.gov/publications/dictionaries/cancer-terms\r\r\rPractice\r\r\r","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578957646,"objectID":"689ae0d83c0f37774ff128d4fd9d9d42","permalink":"/post/vocabulary-for-chapter-1/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/post/vocabulary-for-chapter-1/","section":"post","summary":"Chapter 1 covers generative modeling for discrete data. It introduces a number of terms covering\rprobablity and statistical modeling, as well as a few biological terms. The vocabulary words for\rChapter 1 are:\n\r\r\r\r\r\r\r\rprobability model\r\rA mathematical description of the possible outcomes of an experiment and the probability of each of those outcomes.\r\r\r\rvector\r\rIn programming, a one-dimensional array of data, all with the same data type.","tags":["vocabulary","Chapter 1"],"title":"Vocabulary for Chapter 1","type":"post"},{"authors":null,"categories":null,"content":"\rThis exercise asks us to interpret and validate the consistency within our clusters of data. To do this, we will employ the silhouette index, which gives us a silhouette value measuring how similar an object is to its own cluster compared to other clusters.\nThe silhouette index is as follows:\n\\[\\displaystyle S(i) = \\frac{B(i) - A(i)}{max_i(A(i), B(i))} \\]\nThe solution to this exercise requires the following R packages to be loaded into your environment.\nRequired Libraries\rlibrary(\u0026quot;cluster\u0026quot;)\rlibrary(dplyr)\rlibrary(ggplot2)\rlibrary(purrr)\r\rPart A\rQuestion 5.1.a asks us to compute the silhouette index for the simdat data that was simulated in Section 5.7. The code is as follows:\nset.seed(1)\rsimdat = lapply(c(0, 8), function(mx) {\rlapply(c(0,8), function(my) {\rtibble(x = rnorm(100, mean = mx, sd = 2),\ry = rnorm(100, mean = my, sd = 2),\rclass = paste(mx, my, sep = \u0026quot;:\u0026quot;))\r}) %\u0026gt;% bind_rows\r}) %\u0026gt;% bind_rows\rsimdatxy = simdat[, c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;)]\rwss = tibble(k = 1:8, value = NA_real_)\rwss$value[1] = sum(scale(simdatxy, scale = FALSE)^2)\rfor (i in 2:nrow(wss)) {\rkm = kmeans(simdatxy, centers = wss$k[i])\rwss$value[i] = sum(km$withinss)\r}\rggplot(wss, aes(x = k, y = value)) + geom_col()\rThe provided code is used to simulate data coming from four separate groups. They use the pipe operator to concatenate four different, randomly generated, data sets. The ggplot2 package is used to take a look at the data as a barchart with the k-means method and k = 4.\npam4 = pam(simdatxy, 4)\rsil = silhouette(pam4, 4, border = NA)\rplot(sil, col=c(\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;purple\u0026quot;), main=\u0026quot;Silhouette\u0026quot;)\rNext up is the code necessary to plot the silhouette index. The “silhouette” function comes from the “cluster” package, and the resulting graph provides an average silhouette width for k = 4 clusters.\nComputing the Silhouette Index\rsummary(sil)$avg.width\r## [1] 0.4985801\r\r\rPart B\rQuestion 5.1.b asks us to change the number of clusters k and assess which k value produces the best silhouette index.\nThe silhouette value is a measure of how similar a cluster is to its own cluster when compared to other clusters. Values can range from -1 to +1. A high value tells us that the object is better matched to its on cluster and more poorly matched to neighboring clusters.\nIn this example, there are a couple of ways to assess which k gives the best silhouette index.One method would be trial and error and determining which k-value produces the highest silhouette index. This method works out for this example, but is impractical for much larger and complex datasets. Included below is the code for testing multiple different k-values and the resulting coefficient values.\npam2 = pam(simdatxy, 2)\rsil2 = silhouette(pam2, 2)\rplot(sil2, col=c(\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;purple\u0026quot;), main=\u0026quot;Silhouette\u0026quot;)\rpam3 = pam(simdatxy, 3)\rsil3 = silhouette(pam3, 3)\rplot(sil3, col=c(\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;purple\u0026quot;), main=\u0026quot;Silhouette\u0026quot;)\rpam4 = pam(simdatxy, 4)\rsil = silhouette(pam4, 4)\rplot(sil, col=c(\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;purple\u0026quot;), main=\u0026quot;Silhouette\u0026quot;)\rpam12 = pam(simdatxy, 12)\rsil12 = silhouette(pam12, 12)\rplot(sil12, col=c(\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;purple\u0026quot;), main=\u0026quot;Silhouette\u0026quot;)\rpam40 = pam(simdatxy, 40)\rsil40 = silhouette(pam40, 40)\rplot(sil40, col=c(\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;purple\u0026quot;), main=\u0026quot;Silhouette\u0026quot;)\rThis trial and error method indicates that the highest silhouette index (that was tested) is achieved with k = 4.\nA different (seemingly more appropriate) method is to write a piece of code that will test a range of k-values automatically. This next piece of code is adapted from Amy Fox and the group that she worked with during class. This is a much more practical method that provides a clear answer of which k gives the best silhouette index.\nk \u0026lt;- c(2:10)\rdf_test \u0026lt;- data.frame()\rfor (i in 2:10){\rpam_run \u0026lt;- pam(simdatxy, i)\rsil_run \u0026lt;- silhouette(pam_run, i)\rrow_to_add \u0026lt;- data.frame(i, width = summary(sil_run)$avg.width)\rdf_test \u0026lt;- rbind(df_test, row_to_add)\r}\rdf_test\r## i width\r## 1 2 0.4067400\r## 2 3 0.4000560\r## 3 4 0.4985801\r## 4 5 0.4401518\r## 5 6 0.3957347\r## 6 7 0.3717875\r## 7 8 0.3699929\r## 8 9 0.3670770\r## 9 10 0.3516570\rggplot(df_test, aes(i, width)) +\rgeom_point() +\rgeom_line() +\rxlab(\u0026quot;k\u0026quot;) +\rylab(\u0026quot;Silhouette Index\u0026quot;) +\rggtitle(\u0026quot;Testing different k values for Silhouette Index\u0026quot;)\rsummary(sil_run)\r## Silhouette of 400 units in 10 clusters from pam(x = simdatxy, k = i) :\r## Cluster sizes and average silhouette widths:\r## 63 38 40 52 33 40 35 33 ## 0.3885059 0.3273800 0.3622990 0.3703291 0.3573781 0.3257945 0.4429236 0.2807700 ## 31 35 ## 0.3944945 0.2335738 ## Individual silhouette widths:\r## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.1778 0.2389 0.3703 0.3517 0.4946 0.6623\rThe result of summary(sil_run) matches the trial and error method, but in a more efficient manner..\n\rPart C\rThe last part of this exercise asks us to repeat by calculating the silhouette index on a uniform (unclustered) data distribution over a range of values.\nHere, a new data set is generated without clustering the randomly genereated data.\nset.seed(1)\rsimdat1 = lapply(c(1), function(mx) {\rlapply(c(1), function(my) {\rtibble(x = rnorm(100, mean = mx, sd = 2),\ry = rnorm(100, mean = my, sd = 2),\rclass = paste(mx, my, sep = \u0026quot;:\u0026quot;))\r}) %\u0026gt;% bind_rows\r}) %\u0026gt;% bind_rows\rsimdatxy1 = simdat1[, c(\u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;)]\rggplot(simdatxy1, aes(x = x, y = y)) +\rgeom_point()\rpam4.1 = pam(simdatxy1, 4)\rsil.1 = silhouette(pam4.1, 4)\rplot(sil.1, col=c(\u0026quot;red\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;purple\u0026quot;), main=\u0026quot;Silhouette\u0026quot;)\rThe average silhouette width is 0.33, which is much lower than the clustered value of 0.50 that we see with the first simulation.\n\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a1bc886f183f4bb0b2f71b0ef1bd82b3","permalink":"/post/2020-03-12-ex5-1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/2020-03-12-ex5-1/","section":"post","summary":"This exercise asks us to interpret and validate the consistency within our clusters of data. To do this, we will employ the silhouette index, which gives us a silhouette value measuring how similar an object is to its own cluster compared to other clusters.\nThe silhouette index is as follows:\n\\[\\displaystyle S(i) = \\frac{B(i) - A(i)}{max_i(A(i), B(i))} \\]\nThe solution to this exercise requires the following R packages to be loaded into your environment.","tags":null,"title":"Exercise Solution for 5.1","type":"post"}]